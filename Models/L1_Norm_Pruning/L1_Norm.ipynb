{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-11T18:12:01.413339Z",
     "iopub.status.busy": "2024-12-11T18:12:01.412584Z",
     "iopub.status.idle": "2024-12-11T18:12:05.993389Z",
     "shell.execute_reply": "2024-12-11T18:12:05.992496Z",
     "shell.execute_reply.started": "2024-12-11T18:12:01.413291Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def print_model_param_nums(model=None, multiply_adds=True):\n",
    "    if model == None:\n",
    "        model = torchvision.models.alexnet()\n",
    "    total = sum([param.nelement() for param in model.parameters()])\n",
    "    print('  + Number of params: %.2fM' % (total / 1e6))\n",
    "\n",
    "def print_model_param_flops(model=None, input_res=224, multiply_adds=True):\n",
    "\n",
    "    prods = {}\n",
    "    def save_hook(name):\n",
    "        def hook_per(self, input, output):\n",
    "            prods[name] = np.prod(input[0].shape)\n",
    "        return hook_per\n",
    "\n",
    "    list_1=[]\n",
    "    def simple_hook(self, input, output):\n",
    "        list_1.append(np.prod(input[0].shape))\n",
    "    list_2={}\n",
    "    def simple_hook2(self, input, output):\n",
    "        list_2['names'] = np.prod(input[0].shape)\n",
    "\n",
    "    list_conv=[]\n",
    "    def conv_hook(self, input, output):\n",
    "        batch_size, input_channels, input_height, input_width = input[0].size()\n",
    "        output_channels, output_height, output_width = output[0].size()\n",
    "\n",
    "        kernel_ops = self.kernel_size[0] * self.kernel_size[1] * (self.in_channels / self.groups)\n",
    "        bias_ops = 1 if self.bias is not None else 0\n",
    "\n",
    "        params = output_channels * (kernel_ops + bias_ops)\n",
    "        flops = (kernel_ops * (2 if multiply_adds else 1) + bias_ops) * output_channels * output_height * output_width * batch_size\n",
    "\n",
    "        list_conv.append(flops)\n",
    "\n",
    "    list_linear=[]\n",
    "    def linear_hook(self, input, output):\n",
    "        batch_size = input[0].size(0) if input[0].dim() == 2 else 1\n",
    "\n",
    "        weight_ops = self.weight.nelement() * (2 if multiply_adds else 1)\n",
    "        bias_ops = self.bias.nelement()\n",
    "\n",
    "        flops = batch_size * (weight_ops + bias_ops)\n",
    "        list_linear.append(flops)\n",
    "\n",
    "    list_bn=[]\n",
    "    def bn_hook(self, input, output):\n",
    "        list_bn.append(input[0].nelement() * 2)\n",
    "\n",
    "    list_relu=[]\n",
    "    def relu_hook(self, input, output):\n",
    "        list_relu.append(input[0].nelement())\n",
    "\n",
    "    list_pooling=[]\n",
    "    def pooling_hook(self, input, output):\n",
    "        batch_size, input_channels, input_height, input_width = input[0].size()\n",
    "        output_channels, output_height, output_width = output[0].size()\n",
    "\n",
    "        kernel_ops = self.kernel_size * self.kernel_size\n",
    "        bias_ops = 0\n",
    "        params = 0\n",
    "        flops = (kernel_ops + bias_ops) * output_channels * output_height * output_width * batch_size\n",
    "\n",
    "        list_pooling.append(flops)\n",
    "\n",
    "    list_upsample=[]\n",
    "    # For bilinear upsample\n",
    "    def upsample_hook(self, input, output):\n",
    "        batch_size, input_channels, input_height, input_width = input[0].size()\n",
    "        output_channels, output_height, output_width = output[0].size()\n",
    "\n",
    "        flops = output_height * output_width * output_channels * batch_size * 12\n",
    "        list_upsample.append(flops)\n",
    "\n",
    "    def foo(net):\n",
    "        childrens = list(net.children())\n",
    "        if not childrens:\n",
    "            if isinstance(net, torch.nn.Conv2d):\n",
    "                net.register_forward_hook(conv_hook)\n",
    "            if isinstance(net, torch.nn.Linear):\n",
    "                net.register_forward_hook(linear_hook)\n",
    "            if isinstance(net, torch.nn.BatchNorm2d):\n",
    "                net.register_forward_hook(bn_hook)\n",
    "            if isinstance(net, torch.nn.ReLU):\n",
    "                net.register_forward_hook(relu_hook)\n",
    "            if isinstance(net, torch.nn.MaxPool2d) or isinstance(net, torch.nn.AvgPool2d):\n",
    "                net.register_forward_hook(pooling_hook)\n",
    "            if isinstance(net, torch.nn.Upsample):\n",
    "                net.register_forward_hook(upsample_hook)\n",
    "            return\n",
    "        for c in childrens:\n",
    "            foo(c)\n",
    "\n",
    "    if model == None:\n",
    "        model = torchvision.models.alexnet()\n",
    "    foo(model)\n",
    "    input = Variable(torch.rand(3, 3, input_res, input_res), requires_grad = True)\n",
    "    out = model(input)\n",
    "\n",
    "\n",
    "    total_flops = (sum(list_conv) + sum(list_linear) + sum(list_bn) + sum(list_relu) + sum(list_pooling) + sum(list_upsample))\n",
    "\n",
    "    print('  + Number of FLOPs: %.5fG' % (total_flops / 3 / 1e9))\n",
    "    \n",
    "    return total_flops / 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:12:05.995234Z",
     "iopub.status.busy": "2024-12-11T18:12:05.994902Z",
     "iopub.status.idle": "2024-12-11T18:12:05.999412Z",
     "shell.execute_reply": "2024-12-11T18:12:05.998596Z",
     "shell.execute_reply.started": "2024-12-11T18:12:05.995209Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:12:06.001108Z",
     "iopub.status.busy": "2024-12-11T18:12:06.000693Z",
     "iopub.status.idle": "2024-12-11T18:12:06.086305Z",
     "shell.execute_reply": "2024-12-11T18:12:06.085697Z",
     "shell.execute_reply.started": "2024-12-11T18:12:06.001061Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "__all__ = ['resnet']\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, cfg, stride=1, downsample=None):\n",
    "        # cfg should be a number in this case\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, cfg, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(cfg)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(cfg, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "def downsample_basic_block(x, planes):\n",
    "    x = nn.AvgPool2d(2,2)(x)\n",
    "    zero_pads = torch.Tensor(\n",
    "        x.size(0), planes - x.size(1), x.size(2), x.size(3)).zero_()\n",
    "    if isinstance(x.data, torch.cuda.FloatTensor):\n",
    "        zero_pads = zero_pads.cuda()\n",
    "\n",
    "    out = Variable(torch.cat([x.data, zero_pads], dim=1))\n",
    "\n",
    "    return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, depth, dataset='cifar10', cfg=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        # Model type specifies number of layers for CIFAR-10 model\n",
    "        assert (depth - 2) % 6 == 0, 'depth should be 6n+2'\n",
    "        n = (depth - 2) // 6\n",
    "\n",
    "        block = BasicBlock\n",
    "        if cfg == None:\n",
    "            cfg = [[16]*n, [32]*n, [64]*n]\n",
    "            cfg = [item for sub_list in cfg for item in sub_list]\n",
    "\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.inplanes = 16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 16, n, cfg=cfg[0:n])\n",
    "        self.layer2 = self._make_layer(block, 32, n, cfg=cfg[n:2*n], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, n, cfg=cfg[2*n:3*n], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(8)\n",
    "        if dataset == 'cifar10':\n",
    "            num_classes = 10\n",
    "        elif dataset == 'cifar100':\n",
    "            num_classes = 100\n",
    "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, cfg, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = partial(downsample_basic_block, planes=planes*block.expansion)\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, cfg[0], stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, cfg[i]))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)    # 32x32\n",
    "\n",
    "        x = self.layer1(x)  # 32x32\n",
    "        x = self.layer2(x)  # 16x16\n",
    "        x = self.layer3(x)  # 8x8\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def resnet(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a ResNet model.\n",
    "    \"\"\"\n",
    "    return ResNet(**kwargs)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    net = resnet(depth=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:12:51.551535Z",
     "iopub.status.busy": "2024-12-11T18:12:51.551181Z",
     "iopub.status.idle": "2024-12-11T18:12:51.556235Z",
     "shell.execute_reply": "2024-12-11T18:12:51.555344Z",
     "shell.execute_reply.started": "2024-12-11T18:12:51.551502Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = 'cifar100'\n",
    "depth = 50\n",
    "batch_size = 64\n",
    "test_batch_size = 256\n",
    "epochs = 50\n",
    "lr = 0.001\n",
    "log_interval = 100\n",
    "save = '/kaggle/working/'\n",
    "arch = 'resnet'\n",
    "seed = 1\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:12:52.288204Z",
     "iopub.status.busy": "2024-12-11T18:12:52.287948Z",
     "iopub.status.idle": "2024-12-11T18:12:53.812427Z",
     "shell.execute_reply": "2024-12-11T18:12:53.811760Z",
     "shell.execute_reply.started": "2024-12-11T18:12:52.288180Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR100('./data.cifar100', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.Pad(4),\n",
    "                           transforms.RandomCrop(32),\n",
    "                           transforms.RandomHorizontalFlip(),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR100('./data.cifar100', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "                       ])),\n",
    "        batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:12:55.232201Z",
     "iopub.status.busy": "2024-12-11T18:12:55.231422Z",
     "iopub.status.idle": "2024-12-11T18:12:55.260537Z",
     "shell.execute_reply": "2024-12-11T18:12:55.259912Z",
     "shell.execute_reply.started": "2024-12-11T18:12:55.232166Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = resnet(dataset=dataset, depth=depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:12:59.570275Z",
     "iopub.status.busy": "2024-12-11T18:12:59.569581Z",
     "iopub.status.idle": "2024-12-11T18:12:59.659922Z",
     "shell.execute_reply": "2024-12-11T18:12:59.658889Z",
     "shell.execute_reply.started": "2024-12-11T18:12:59.570235Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if not os.path.exists(save):\n",
    "    os.makedirs(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-11T18:13:01.299110Z",
     "iopub.status.busy": "2024-12-11T18:13:01.298206Z",
     "iopub.status.idle": "2024-12-11T18:13:01.496173Z",
     "shell.execute_reply": "2024-12-11T18:13:01.495346Z",
     "shell.execute_reply.started": "2024-12-11T18:13:01.299075Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (6): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (7): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (6): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (7): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (6): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (7): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "  (fc): Linear(in_features=64, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:13:09.573853Z",
     "iopub.status.busy": "2024-12-11T18:13:09.573154Z",
     "iopub.status.idle": "2024-12-11T18:13:09.578557Z",
     "shell.execute_reply": "2024-12-11T18:13:09.577632Z",
     "shell.execute_reply.started": "2024-12-11T18:13:09.573820Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:13:11.369414Z",
     "iopub.status.busy": "2024-12-11T18:13:11.368853Z",
     "iopub.status.idle": "2024-12-11T18:13:11.375870Z",
     "shell.execute_reply": "2024-12-11T18:13:11.374999Z",
     "shell.execute_reply.started": "2024-12-11T18:13:11.369381Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "    train_acc = 0.\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: Compute model output\n",
    "        output = model(data)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        avg_loss = avg_loss + loss.item()\n",
    "        \n",
    "        # Use torch.no_grad() for accuracy computation (no gradients required here)\n",
    "        with torch.no_grad():\n",
    "            pred = output.data.max(1, keepdim=True)[1]  # Get index of max log-probability\n",
    "            train_acc += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "        # Backpropagate the loss and optimize the model\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log progress\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.1f}%)]\\tLoss: {loss.item():.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:16:46.239175Z",
     "iopub.status.busy": "2024-12-11T18:16:46.238497Z",
     "iopub.status.idle": "2024-12-11T18:16:46.245666Z",
     "shell.execute_reply": "2024-12-11T18:16:46.244741Z",
     "shell.execute_reply.started": "2024-12-11T18:16:46.239142Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.profiler import profile, ProfilerActivity\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    # Start profiling\n",
    "    with profile(\n",
    "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True,\n",
    "    ) as prof:  # Profiler object initialized here\n",
    "        with torch.no_grad():  # Use torch.no_grad() for inference\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                output = model(data)\n",
    "                test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum batch loss\n",
    "                pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    prof.export_chrome_trace(\"trace.json\")\n",
    "\n",
    "    # Calculate test loss and accuracy\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "    return correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:20:16.266095Z",
     "iopub.status.busy": "2024-12-11T18:20:16.265402Z",
     "iopub.status.idle": "2024-12-11T18:20:16.270766Z",
     "shell.execute_reply": "2024-12-11T18:20:16.269769Z",
     "shell.execute_reply.started": "2024-12-11T18:20:16.266062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filepath):\n",
    "    torch.save(state, os.path.join(filepath, 'checkpoint.pth.tar'))\n",
    "    if is_best:\n",
    "        shutil.copyfile(os.path.join(filepath, 'checkpoint.pth.tar'), os.path.join(filepath, 'model_best.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-11T19:49:00.049555Z",
     "iopub.status.busy": "2024-12-11T19:49:00.049219Z",
     "iopub.status.idle": "2024-12-11T20:45:11.124491Z",
     "shell.execute_reply": "2024-12-11T20:45:11.123582Z",
     "shell.execute_reply.started": "2024-12-11T19:49:00.049522Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/50000 (0.0%)]\tLoss: 2.543618\n",
      "Train Epoch: 0 [6400/50000 (12.8%)]\tLoss: 2.356167\n",
      "Train Epoch: 0 [12800/50000 (25.6%)]\tLoss: 2.022959\n",
      "Train Epoch: 0 [19200/50000 (38.4%)]\tLoss: 2.233490\n",
      "Train Epoch: 0 [25600/50000 (51.2%)]\tLoss: 1.847636\n",
      "Train Epoch: 0 [32000/50000 (63.9%)]\tLoss: 2.182014\n",
      "Train Epoch: 0 [38400/50000 (76.7%)]\tLoss: 2.446960\n",
      "Train Epoch: 0 [44800/50000 (89.5%)]\tLoss: 2.261799\n",
      "\n",
      "Test set: Average loss: 2.3250, Accuracy: 3917/10000 (39.2%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0.0%)]\tLoss: 2.235600\n",
      "Train Epoch: 1 [6400/50000 (12.8%)]\tLoss: 2.391966\n",
      "Train Epoch: 1 [12800/50000 (25.6%)]\tLoss: 2.266216\n",
      "Train Epoch: 1 [19200/50000 (38.4%)]\tLoss: 2.073068\n",
      "Train Epoch: 1 [25600/50000 (51.2%)]\tLoss: 2.209569\n",
      "Train Epoch: 1 [32000/50000 (63.9%)]\tLoss: 2.207880\n",
      "Train Epoch: 1 [38400/50000 (76.7%)]\tLoss: 2.016623\n",
      "Train Epoch: 1 [44800/50000 (89.5%)]\tLoss: 2.083930\n",
      "\n",
      "Test set: Average loss: 2.3147, Accuracy: 3956/10000 (39.6%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0.0%)]\tLoss: 2.392545\n",
      "Train Epoch: 2 [6400/50000 (12.8%)]\tLoss: 2.416978\n",
      "Train Epoch: 2 [12800/50000 (25.6%)]\tLoss: 2.066119\n",
      "Train Epoch: 2 [19200/50000 (38.4%)]\tLoss: 2.283969\n",
      "Train Epoch: 2 [25600/50000 (51.2%)]\tLoss: 2.396939\n",
      "Train Epoch: 2 [32000/50000 (63.9%)]\tLoss: 1.897433\n",
      "Train Epoch: 2 [38400/50000 (76.7%)]\tLoss: 2.282701\n",
      "Train Epoch: 2 [44800/50000 (89.5%)]\tLoss: 2.240623\n",
      "\n",
      "Test set: Average loss: 2.3161, Accuracy: 3973/10000 (39.7%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0.0%)]\tLoss: 2.374919\n",
      "Train Epoch: 3 [6400/50000 (12.8%)]\tLoss: 2.215544\n",
      "Train Epoch: 3 [12800/50000 (25.6%)]\tLoss: 2.157560\n",
      "Train Epoch: 3 [19200/50000 (38.4%)]\tLoss: 2.686341\n",
      "Train Epoch: 3 [25600/50000 (51.2%)]\tLoss: 2.394203\n",
      "Train Epoch: 3 [32000/50000 (63.9%)]\tLoss: 2.299894\n",
      "Train Epoch: 3 [38400/50000 (76.7%)]\tLoss: 2.216002\n",
      "Train Epoch: 3 [44800/50000 (89.5%)]\tLoss: 1.901262\n",
      "\n",
      "Test set: Average loss: 2.3094, Accuracy: 3958/10000 (39.6%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0.0%)]\tLoss: 2.339190\n",
      "Train Epoch: 4 [6400/50000 (12.8%)]\tLoss: 2.189086\n",
      "Train Epoch: 4 [12800/50000 (25.6%)]\tLoss: 2.243670\n",
      "Train Epoch: 4 [19200/50000 (38.4%)]\tLoss: 2.207862\n",
      "Train Epoch: 4 [25600/50000 (51.2%)]\tLoss: 2.071799\n",
      "Train Epoch: 4 [32000/50000 (63.9%)]\tLoss: 2.621659\n",
      "Train Epoch: 4 [38400/50000 (76.7%)]\tLoss: 2.128166\n",
      "Train Epoch: 4 [44800/50000 (89.5%)]\tLoss: 2.493495\n",
      "\n",
      "Test set: Average loss: 2.3126, Accuracy: 3978/10000 (39.8%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0.0%)]\tLoss: 2.262972\n",
      "Train Epoch: 5 [6400/50000 (12.8%)]\tLoss: 2.520895\n",
      "Train Epoch: 5 [12800/50000 (25.6%)]\tLoss: 1.978102\n",
      "Train Epoch: 5 [19200/50000 (38.4%)]\tLoss: 2.134921\n",
      "Train Epoch: 5 [25600/50000 (51.2%)]\tLoss: 2.097858\n",
      "Train Epoch: 5 [32000/50000 (63.9%)]\tLoss: 2.106299\n",
      "Train Epoch: 5 [38400/50000 (76.7%)]\tLoss: 2.258677\n",
      "Train Epoch: 5 [44800/50000 (89.5%)]\tLoss: 2.318828\n",
      "\n",
      "Test set: Average loss: 2.3232, Accuracy: 3940/10000 (39.4%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0.0%)]\tLoss: 2.142276\n",
      "Train Epoch: 6 [6400/50000 (12.8%)]\tLoss: 2.028840\n",
      "Train Epoch: 6 [12800/50000 (25.6%)]\tLoss: 2.475535\n",
      "Train Epoch: 6 [19200/50000 (38.4%)]\tLoss: 2.106112\n",
      "Train Epoch: 6 [25600/50000 (51.2%)]\tLoss: 2.406470\n",
      "Train Epoch: 6 [32000/50000 (63.9%)]\tLoss: 2.132973\n",
      "Train Epoch: 6 [38400/50000 (76.7%)]\tLoss: 2.348104\n",
      "Train Epoch: 6 [44800/50000 (89.5%)]\tLoss: 2.286016\n",
      "\n",
      "Test set: Average loss: 2.3070, Accuracy: 3971/10000 (39.7%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0.0%)]\tLoss: 2.172905\n",
      "Train Epoch: 7 [6400/50000 (12.8%)]\tLoss: 2.095552\n",
      "Train Epoch: 7 [12800/50000 (25.6%)]\tLoss: 2.823892\n",
      "Train Epoch: 7 [19200/50000 (38.4%)]\tLoss: 2.383181\n",
      "Train Epoch: 7 [25600/50000 (51.2%)]\tLoss: 2.396900\n",
      "Train Epoch: 7 [32000/50000 (63.9%)]\tLoss: 2.320127\n",
      "Train Epoch: 7 [38400/50000 (76.7%)]\tLoss: 1.917948\n",
      "Train Epoch: 7 [44800/50000 (89.5%)]\tLoss: 2.433429\n",
      "\n",
      "Test set: Average loss: 2.3122, Accuracy: 3970/10000 (39.7%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0.0%)]\tLoss: 2.306085\n",
      "Train Epoch: 8 [6400/50000 (12.8%)]\tLoss: 2.070146\n",
      "Train Epoch: 8 [12800/50000 (25.6%)]\tLoss: 2.672326\n",
      "Train Epoch: 8 [19200/50000 (38.4%)]\tLoss: 2.604494\n",
      "Train Epoch: 8 [25600/50000 (51.2%)]\tLoss: 1.971486\n",
      "Train Epoch: 8 [32000/50000 (63.9%)]\tLoss: 1.949852\n",
      "Train Epoch: 8 [38400/50000 (76.7%)]\tLoss: 2.502379\n",
      "Train Epoch: 8 [44800/50000 (89.5%)]\tLoss: 2.070896\n",
      "\n",
      "Test set: Average loss: 2.3084, Accuracy: 4009/10000 (40.1%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0.0%)]\tLoss: 2.140803\n",
      "Train Epoch: 9 [6400/50000 (12.8%)]\tLoss: 2.134060\n",
      "Train Epoch: 9 [12800/50000 (25.6%)]\tLoss: 2.108783\n",
      "Train Epoch: 9 [19200/50000 (38.4%)]\tLoss: 2.270175\n",
      "Train Epoch: 9 [25600/50000 (51.2%)]\tLoss: 2.551533\n",
      "Train Epoch: 9 [32000/50000 (63.9%)]\tLoss: 2.187495\n",
      "Train Epoch: 9 [38400/50000 (76.7%)]\tLoss: 1.993656\n",
      "Train Epoch: 9 [44800/50000 (89.5%)]\tLoss: 2.030884\n",
      "\n",
      "Test set: Average loss: 2.3017, Accuracy: 3957/10000 (39.6%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0.0%)]\tLoss: 2.010856\n",
      "Train Epoch: 10 [6400/50000 (12.8%)]\tLoss: 2.073479\n",
      "Train Epoch: 10 [12800/50000 (25.6%)]\tLoss: 1.955018\n",
      "Train Epoch: 10 [19200/50000 (38.4%)]\tLoss: 2.188585\n",
      "Train Epoch: 10 [25600/50000 (51.2%)]\tLoss: 1.968071\n",
      "Train Epoch: 10 [32000/50000 (63.9%)]\tLoss: 2.189653\n",
      "Train Epoch: 10 [38400/50000 (76.7%)]\tLoss: 1.834236\n",
      "Train Epoch: 10 [44800/50000 (89.5%)]\tLoss: 2.079348\n",
      "\n",
      "Test set: Average loss: 2.3001, Accuracy: 3983/10000 (39.8%)\n",
      "\n",
      "Train Epoch: 11 [0/50000 (0.0%)]\tLoss: 2.659591\n",
      "Train Epoch: 11 [6400/50000 (12.8%)]\tLoss: 2.322770\n",
      "Train Epoch: 11 [12800/50000 (25.6%)]\tLoss: 2.227674\n",
      "Train Epoch: 11 [19200/50000 (38.4%)]\tLoss: 1.942214\n",
      "Train Epoch: 11 [25600/50000 (51.2%)]\tLoss: 2.324064\n",
      "Train Epoch: 11 [32000/50000 (63.9%)]\tLoss: 2.316892\n",
      "Train Epoch: 11 [38400/50000 (76.7%)]\tLoss: 1.970919\n",
      "Train Epoch: 11 [44800/50000 (89.5%)]\tLoss: 1.791339\n",
      "\n",
      "Test set: Average loss: 2.2896, Accuracy: 4016/10000 (40.2%)\n",
      "\n",
      "Train Epoch: 12 [0/50000 (0.0%)]\tLoss: 2.489003\n",
      "Train Epoch: 12 [6400/50000 (12.8%)]\tLoss: 1.835101\n",
      "Train Epoch: 12 [12800/50000 (25.6%)]\tLoss: 2.344476\n",
      "Train Epoch: 12 [19200/50000 (38.4%)]\tLoss: 2.258548\n",
      "Train Epoch: 12 [25600/50000 (51.2%)]\tLoss: 2.139576\n",
      "Train Epoch: 12 [32000/50000 (63.9%)]\tLoss: 2.337547\n",
      "Train Epoch: 12 [38400/50000 (76.7%)]\tLoss: 2.099741\n",
      "Train Epoch: 12 [44800/50000 (89.5%)]\tLoss: 2.131285\n",
      "\n",
      "Test set: Average loss: 2.2899, Accuracy: 4001/10000 (40.0%)\n",
      "\n",
      "Train Epoch: 13 [0/50000 (0.0%)]\tLoss: 1.969237\n",
      "Train Epoch: 13 [6400/50000 (12.8%)]\tLoss: 2.020152\n",
      "Train Epoch: 13 [12800/50000 (25.6%)]\tLoss: 2.160826\n",
      "Train Epoch: 13 [19200/50000 (38.4%)]\tLoss: 2.150583\n",
      "Train Epoch: 13 [25600/50000 (51.2%)]\tLoss: 2.189523\n",
      "Train Epoch: 13 [32000/50000 (63.9%)]\tLoss: 2.461690\n",
      "Train Epoch: 13 [38400/50000 (76.7%)]\tLoss: 2.270323\n",
      "Train Epoch: 13 [44800/50000 (89.5%)]\tLoss: 2.187771\n",
      "\n",
      "Test set: Average loss: 2.2896, Accuracy: 4032/10000 (40.3%)\n",
      "\n",
      "Train Epoch: 14 [0/50000 (0.0%)]\tLoss: 2.169048\n",
      "Train Epoch: 14 [6400/50000 (12.8%)]\tLoss: 2.355263\n",
      "Train Epoch: 14 [12800/50000 (25.6%)]\tLoss: 2.400355\n",
      "Train Epoch: 14 [19200/50000 (38.4%)]\tLoss: 2.190582\n",
      "Train Epoch: 14 [25600/50000 (51.2%)]\tLoss: 2.064142\n",
      "Train Epoch: 14 [32000/50000 (63.9%)]\tLoss: 2.469285\n",
      "Train Epoch: 14 [38400/50000 (76.7%)]\tLoss: 2.142257\n",
      "Train Epoch: 14 [44800/50000 (89.5%)]\tLoss: 2.186275\n",
      "\n",
      "Test set: Average loss: 2.2888, Accuracy: 4013/10000 (40.1%)\n",
      "\n",
      "Train Epoch: 15 [0/50000 (0.0%)]\tLoss: 2.187715\n",
      "Train Epoch: 15 [6400/50000 (12.8%)]\tLoss: 1.817571\n",
      "Train Epoch: 15 [12800/50000 (25.6%)]\tLoss: 2.157754\n",
      "Train Epoch: 15 [19200/50000 (38.4%)]\tLoss: 2.214176\n",
      "Train Epoch: 15 [25600/50000 (51.2%)]\tLoss: 2.273292\n",
      "Train Epoch: 15 [32000/50000 (63.9%)]\tLoss: 2.464381\n",
      "Train Epoch: 15 [38400/50000 (76.7%)]\tLoss: 1.690253\n",
      "Train Epoch: 15 [44800/50000 (89.5%)]\tLoss: 2.629501\n",
      "\n",
      "Test set: Average loss: 2.2885, Accuracy: 4025/10000 (40.2%)\n",
      "\n",
      "Train Epoch: 16 [0/50000 (0.0%)]\tLoss: 2.252517\n",
      "Train Epoch: 16 [6400/50000 (12.8%)]\tLoss: 2.040786\n",
      "Train Epoch: 16 [12800/50000 (25.6%)]\tLoss: 2.250762\n",
      "Train Epoch: 16 [19200/50000 (38.4%)]\tLoss: 2.060722\n",
      "Train Epoch: 16 [25600/50000 (51.2%)]\tLoss: 2.463812\n",
      "Train Epoch: 16 [32000/50000 (63.9%)]\tLoss: 2.176872\n",
      "Train Epoch: 16 [38400/50000 (76.7%)]\tLoss: 2.265887\n",
      "Train Epoch: 16 [44800/50000 (89.5%)]\tLoss: 2.048665\n",
      "\n",
      "Test set: Average loss: 2.2880, Accuracy: 4021/10000 (40.2%)\n",
      "\n",
      "Train Epoch: 17 [0/50000 (0.0%)]\tLoss: 1.854636\n",
      "Train Epoch: 17 [6400/50000 (12.8%)]\tLoss: 2.331953\n",
      "Train Epoch: 17 [12800/50000 (25.6%)]\tLoss: 2.196956\n",
      "Train Epoch: 17 [19200/50000 (38.4%)]\tLoss: 2.172415\n",
      "Train Epoch: 17 [25600/50000 (51.2%)]\tLoss: 1.935565\n",
      "Train Epoch: 17 [32000/50000 (63.9%)]\tLoss: 2.000706\n",
      "Train Epoch: 17 [38400/50000 (76.7%)]\tLoss: 1.826452\n",
      "Train Epoch: 17 [44800/50000 (89.5%)]\tLoss: 2.181593\n",
      "\n",
      "Test set: Average loss: 2.2863, Accuracy: 4022/10000 (40.2%)\n",
      "\n",
      "Train Epoch: 18 [0/50000 (0.0%)]\tLoss: 2.122971\n",
      "Train Epoch: 18 [6400/50000 (12.8%)]\tLoss: 1.621931\n",
      "Train Epoch: 18 [12800/50000 (25.6%)]\tLoss: 2.284756\n",
      "Train Epoch: 18 [19200/50000 (38.4%)]\tLoss: 2.295781\n",
      "Train Epoch: 18 [25600/50000 (51.2%)]\tLoss: 2.129515\n",
      "Train Epoch: 18 [32000/50000 (63.9%)]\tLoss: 2.248591\n",
      "Train Epoch: 18 [38400/50000 (76.7%)]\tLoss: 2.057922\n",
      "Train Epoch: 18 [44800/50000 (89.5%)]\tLoss: 2.073982\n",
      "\n",
      "Test set: Average loss: 2.2841, Accuracy: 4013/10000 (40.1%)\n",
      "\n",
      "Train Epoch: 19 [0/50000 (0.0%)]\tLoss: 2.412677\n",
      "Train Epoch: 19 [6400/50000 (12.8%)]\tLoss: 1.759558\n",
      "Train Epoch: 19 [12800/50000 (25.6%)]\tLoss: 1.807233\n",
      "Train Epoch: 19 [19200/50000 (38.4%)]\tLoss: 2.091043\n",
      "Train Epoch: 19 [25600/50000 (51.2%)]\tLoss: 2.151927\n",
      "Train Epoch: 19 [32000/50000 (63.9%)]\tLoss: 2.071786\n",
      "Train Epoch: 19 [38400/50000 (76.7%)]\tLoss: 2.268009\n",
      "Train Epoch: 19 [44800/50000 (89.5%)]\tLoss: 2.060137\n",
      "\n",
      "Test set: Average loss: 2.2830, Accuracy: 4022/10000 (40.2%)\n",
      "\n",
      "Train Epoch: 20 [0/50000 (0.0%)]\tLoss: 2.249903\n",
      "Train Epoch: 20 [6400/50000 (12.8%)]\tLoss: 2.278198\n",
      "Train Epoch: 20 [12800/50000 (25.6%)]\tLoss: 1.955894\n",
      "Train Epoch: 20 [19200/50000 (38.4%)]\tLoss: 1.995664\n",
      "Train Epoch: 20 [25600/50000 (51.2%)]\tLoss: 2.388871\n",
      "Train Epoch: 20 [32000/50000 (63.9%)]\tLoss: 2.348842\n",
      "Train Epoch: 20 [38400/50000 (76.7%)]\tLoss: 2.170775\n",
      "Train Epoch: 20 [44800/50000 (89.5%)]\tLoss: 1.820435\n",
      "\n",
      "Test set: Average loss: 2.2946, Accuracy: 3986/10000 (39.9%)\n",
      "\n",
      "Train Epoch: 21 [0/50000 (0.0%)]\tLoss: 2.474350\n",
      "Train Epoch: 21 [6400/50000 (12.8%)]\tLoss: 2.055235\n",
      "Train Epoch: 21 [12800/50000 (25.6%)]\tLoss: 2.187915\n",
      "Train Epoch: 21 [19200/50000 (38.4%)]\tLoss: 2.131962\n",
      "Train Epoch: 21 [25600/50000 (51.2%)]\tLoss: 2.657068\n",
      "Train Epoch: 21 [32000/50000 (63.9%)]\tLoss: 2.453484\n",
      "Train Epoch: 21 [38400/50000 (76.7%)]\tLoss: 1.736795\n",
      "Train Epoch: 21 [44800/50000 (89.5%)]\tLoss: 2.239068\n",
      "\n",
      "Test set: Average loss: 2.2908, Accuracy: 4018/10000 (40.2%)\n",
      "\n",
      "Train Epoch: 22 [0/50000 (0.0%)]\tLoss: 1.975549\n",
      "Train Epoch: 22 [6400/50000 (12.8%)]\tLoss: 2.285310\n",
      "Train Epoch: 22 [12800/50000 (25.6%)]\tLoss: 2.573843\n",
      "Train Epoch: 22 [19200/50000 (38.4%)]\tLoss: 2.331599\n",
      "Train Epoch: 22 [25600/50000 (51.2%)]\tLoss: 2.214497\n",
      "Train Epoch: 22 [32000/50000 (63.9%)]\tLoss: 2.334501\n",
      "Train Epoch: 22 [38400/50000 (76.7%)]\tLoss: 2.332083\n",
      "Train Epoch: 22 [44800/50000 (89.5%)]\tLoss: 1.865567\n",
      "\n",
      "Test set: Average loss: 2.2882, Accuracy: 4036/10000 (40.4%)\n",
      "\n",
      "Train Epoch: 23 [0/50000 (0.0%)]\tLoss: 2.104058\n",
      "Train Epoch: 23 [6400/50000 (12.8%)]\tLoss: 2.343382\n",
      "Train Epoch: 23 [12800/50000 (25.6%)]\tLoss: 2.075811\n",
      "Train Epoch: 23 [19200/50000 (38.4%)]\tLoss: 2.432890\n",
      "Train Epoch: 23 [25600/50000 (51.2%)]\tLoss: 2.207827\n",
      "Train Epoch: 23 [32000/50000 (63.9%)]\tLoss: 2.362819\n",
      "Train Epoch: 23 [38400/50000 (76.7%)]\tLoss: 2.260664\n",
      "Train Epoch: 23 [44800/50000 (89.5%)]\tLoss: 2.489171\n",
      "\n",
      "Test set: Average loss: 2.2836, Accuracy: 4007/10000 (40.1%)\n",
      "\n",
      "Train Epoch: 24 [0/50000 (0.0%)]\tLoss: 2.100467\n",
      "Train Epoch: 24 [6400/50000 (12.8%)]\tLoss: 2.154853\n",
      "Train Epoch: 24 [12800/50000 (25.6%)]\tLoss: 2.051914\n",
      "Train Epoch: 24 [19200/50000 (38.4%)]\tLoss: 1.980183\n",
      "Train Epoch: 24 [25600/50000 (51.2%)]\tLoss: 2.226873\n",
      "Train Epoch: 24 [32000/50000 (63.9%)]\tLoss: 2.667074\n",
      "Train Epoch: 24 [38400/50000 (76.7%)]\tLoss: 1.994736\n",
      "Train Epoch: 24 [44800/50000 (89.5%)]\tLoss: 2.173734\n",
      "\n",
      "Test set: Average loss: 2.2866, Accuracy: 4008/10000 (40.1%)\n",
      "\n",
      "Train Epoch: 25 [0/50000 (0.0%)]\tLoss: 2.463719\n",
      "Train Epoch: 25 [6400/50000 (12.8%)]\tLoss: 2.223489\n",
      "Train Epoch: 25 [12800/50000 (25.6%)]\tLoss: 2.101021\n",
      "Train Epoch: 25 [19200/50000 (38.4%)]\tLoss: 2.221565\n",
      "Train Epoch: 25 [25600/50000 (51.2%)]\tLoss: 2.064741\n",
      "Train Epoch: 25 [32000/50000 (63.9%)]\tLoss: 1.934474\n",
      "Train Epoch: 25 [38400/50000 (76.7%)]\tLoss: 2.388413\n",
      "Train Epoch: 25 [44800/50000 (89.5%)]\tLoss: 2.602338\n",
      "\n",
      "Test set: Average loss: 2.2827, Accuracy: 4013/10000 (40.1%)\n",
      "\n",
      "Train Epoch: 26 [0/50000 (0.0%)]\tLoss: 2.283065\n",
      "Train Epoch: 26 [6400/50000 (12.8%)]\tLoss: 2.397952\n",
      "Train Epoch: 26 [12800/50000 (25.6%)]\tLoss: 2.262341\n",
      "Train Epoch: 26 [19200/50000 (38.4%)]\tLoss: 2.398496\n",
      "Train Epoch: 26 [25600/50000 (51.2%)]\tLoss: 2.439741\n",
      "Train Epoch: 26 [32000/50000 (63.9%)]\tLoss: 2.195794\n",
      "Train Epoch: 26 [38400/50000 (76.7%)]\tLoss: 2.076111\n",
      "Train Epoch: 26 [44800/50000 (89.5%)]\tLoss: 2.293691\n",
      "\n",
      "Test set: Average loss: 2.2845, Accuracy: 3990/10000 (39.9%)\n",
      "\n",
      "Train Epoch: 27 [0/50000 (0.0%)]\tLoss: 2.340602\n",
      "Train Epoch: 27 [6400/50000 (12.8%)]\tLoss: 2.454646\n",
      "Train Epoch: 27 [12800/50000 (25.6%)]\tLoss: 2.468737\n",
      "Train Epoch: 27 [19200/50000 (38.4%)]\tLoss: 2.196427\n",
      "Train Epoch: 27 [25600/50000 (51.2%)]\tLoss: 2.341210\n",
      "Train Epoch: 27 [32000/50000 (63.9%)]\tLoss: 2.044773\n",
      "Train Epoch: 27 [38400/50000 (76.7%)]\tLoss: 2.256569\n",
      "Train Epoch: 27 [44800/50000 (89.5%)]\tLoss: 2.351135\n",
      "\n",
      "Test set: Average loss: 2.2879, Accuracy: 4022/10000 (40.2%)\n",
      "\n",
      "Train Epoch: 28 [0/50000 (0.0%)]\tLoss: 2.304728\n",
      "Train Epoch: 28 [6400/50000 (12.8%)]\tLoss: 2.344286\n",
      "Train Epoch: 28 [12800/50000 (25.6%)]\tLoss: 2.340494\n",
      "Train Epoch: 28 [19200/50000 (38.4%)]\tLoss: 2.348389\n",
      "Train Epoch: 28 [25600/50000 (51.2%)]\tLoss: 2.370445\n",
      "Train Epoch: 28 [32000/50000 (63.9%)]\tLoss: 2.321330\n",
      "Train Epoch: 28 [38400/50000 (76.7%)]\tLoss: 2.337213\n",
      "Train Epoch: 28 [44800/50000 (89.5%)]\tLoss: 2.061669\n",
      "\n",
      "Test set: Average loss: 2.2925, Accuracy: 3996/10000 (40.0%)\n",
      "\n",
      "Train Epoch: 29 [0/50000 (0.0%)]\tLoss: 2.451488\n",
      "Train Epoch: 29 [6400/50000 (12.8%)]\tLoss: 2.398571\n",
      "Train Epoch: 29 [12800/50000 (25.6%)]\tLoss: 2.957354\n",
      "Train Epoch: 29 [19200/50000 (38.4%)]\tLoss: 2.182836\n",
      "Train Epoch: 29 [25600/50000 (51.2%)]\tLoss: 2.131085\n",
      "Train Epoch: 29 [32000/50000 (63.9%)]\tLoss: 2.453867\n",
      "Train Epoch: 29 [38400/50000 (76.7%)]\tLoss: 1.621795\n",
      "Train Epoch: 29 [44800/50000 (89.5%)]\tLoss: 2.438488\n",
      "\n",
      "Test set: Average loss: 2.2886, Accuracy: 3993/10000 (39.9%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_prec1 = 0\n",
    "epochs = 30\n",
    "for epoch in range(0, epochs):\n",
    "    if epoch in [epochs*0.5, epochs*0.75]:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= 0.001\n",
    "    train(epoch)\n",
    "    prec1 = test()\n",
    "    is_best = prec1 > best_prec1\n",
    "    best_prec1 = max(prec1, best_prec1)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec1': best_prec1,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'cfg': model.cfg\n",
    "    }, is_best, filepath=save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T20:47:43.479382Z",
     "iopub.status.busy": "2024-12-11T20:47:43.479020Z",
     "iopub.status.idle": "2024-12-11T20:47:43.513131Z",
     "shell.execute_reply": "2024-12-11T20:47:43.512321Z",
     "shell.execute_reply.started": "2024-12-11T20:47:43.479350Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /kaggle/working/resnet_model.pth\n"
     ]
    }
   ],
   "source": [
    "if save:\n",
    "    save_path = os.path.join(save, 'resnet_model.pth')\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy and Number of Parameters for the Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "base_model = torch.load('resnet_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print('Accuracy for the Baseline model')\n",
    "acc = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T20:47:44.153208Z",
     "iopub.status.busy": "2024-12-11T20:47:44.152951Z",
     "iopub.status.idle": "2024-12-11T20:47:44.158421Z",
     "shell.execute_reply": "2024-12-11T20:47:44.157600Z",
     "shell.execute_reply.started": "2024-12-11T20:47:44.153183Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "761652\n"
     ]
    }
   ],
   "source": [
    "num_parameters = sum([param.nelement() for param in model.parameters()])\n",
    "print(num_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T20:54:12.849675Z",
     "iopub.status.busy": "2024-12-11T20:54:12.848894Z",
     "iopub.status.idle": "2024-12-11T20:54:12.861312Z",
     "shell.execute_reply": "2024-12-11T20:54:12.860510Z",
     "shell.execute_reply.started": "2024-12-11T20:54:12.849640Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "import torch.quantization as quant\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import time\n",
    "\n",
    "def profile_model(model, data_loader, num_batches=10):\n",
    "    model.eval()\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "\n",
    "    with profile(\n",
    "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./logs'),\n",
    "        record_shapes=True,\n",
    "        with_stack=True\n",
    "    ) as prof:\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            if batch_idx >= num_batches:\n",
    "                break\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            start_time = time.time()\n",
    "            with record_function(\"model_inference\"):\n",
    "                outputs = model(inputs)\n",
    "            latency = time.time() - start_time\n",
    "\n",
    "    print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "    print(f\"\\nLatency per batch: {latency:.4f} seconds\")\n",
    "    print(f\"Throughput: {len(inputs) / latency:.4f} samples/second\")\n",
    "    print(f\"Peak GPU memory usage: {torch.cuda.max_memory_allocated(device) / (1024 * 1024):.2f} MB\")\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model parameter count: {total_params}\")\n",
    "    print(f\"FLOPs estimation: {total_params * 2:.2e} FLOPs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T20:55:28.771073Z",
     "iopub.status.busy": "2024-12-11T20:55:28.770698Z",
     "iopub.status.idle": "2024-12-11T20:55:50.843249Z",
     "shell.execute_reply": "2024-12-11T20:55:50.842308Z",
     "shell.execute_reply.started": "2024-12-11T20:55:28.771038Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference         0.00%       0.000us         0.00%       0.000us       0.000us     454.575ms        50.58%     454.575ms      45.457ms            10  \n",
      "                                        model_inference         4.06%      71.153ms        25.74%     451.576ms      45.158ms       0.000us         0.00%     438.587ms      43.859ms            10  \n",
      "                                           aten::conv2d         0.16%       2.835ms         3.25%      56.972ms     116.269us       0.000us         0.00%     306.257ms     625.014us           490  \n",
      "                                      aten::convolution         0.38%       6.720ms         3.09%      54.137ms     110.484us       0.000us         0.00%     306.257ms     625.014us           490  \n",
      "                                     aten::_convolution         0.26%       4.638ms         2.70%      47.418ms      96.771us       0.000us         0.00%     306.257ms     625.014us           490  \n",
      "                                aten::cudnn_convolution         1.59%      27.965ms         2.44%      42.780ms      87.306us     306.257ms        34.08%     306.257ms     625.014us           490  \n",
      "       cudnn_infer_volta_scudnn_128x32_relu_small_nn_v1         0.00%       0.000us         0.00%       0.000us       0.000us     253.919ms        28.25%     253.919ms     769.452us           330  \n",
      "                                       aten::batch_norm         0.09%       1.620ms         3.08%      53.984ms     110.171us       0.000us         0.00%      50.232ms     102.514us           490  \n",
      "                           aten::_batch_norm_impl_index         0.16%       2.877ms         2.99%      52.364ms     106.866us       0.000us         0.00%      50.232ms     102.514us           490  \n",
      "                                 aten::cudnn_batch_norm         1.28%      22.384ms         2.82%      49.487ms     100.994us      50.232ms         5.59%      50.232ms     102.514us           490  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.754s\n",
      "Self CUDA time total: 898.692ms\n",
      "\n",
      "\n",
      "Latency per batch: 0.0439 seconds\n",
      "Throughput: 5836.1878 samples/second\n",
      "Peak GPU memory usage: 1906.67 MB\n",
      "Model parameter count: 761652\n",
      "FLOPs estimation: 1.52e+06 FLOPs\n"
     ]
    }
   ],
   "source": [
    "profile_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T21:10:01.281513Z",
     "iopub.status.busy": "2024-12-11T21:10:01.281101Z",
     "iopub.status.idle": "2024-12-11T21:10:01.286928Z",
     "shell.execute_reply": "2024-12-11T21:10:01.286029Z",
     "shell.execute_reply.started": "2024-12-11T21:10:01.281479Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "skip = {\n",
    "    'A': [16, 20, 38, 54],\n",
    "    'B': [16, 18, 20, 34, 38, 54],\n",
    "}\n",
    "\n",
    "prune_prob = {\n",
    "    'A': [0.1, 0.1, 0.1],\n",
    "    'B': [0.6, 0.3, 0.1],\n",
    "}\n",
    "\n",
    "layer_id = 1\n",
    "cfg = []\n",
    "cfg_mask = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T21:10:03.201575Z",
     "iopub.status.busy": "2024-12-11T21:10:03.201251Z",
     "iopub.status.idle": "2024-12-11T21:10:03.215097Z",
     "shell.execute_reply": "2024-12-11T21:10:03.214426Z",
     "shell.execute_reply.started": "2024-12-11T21:10:03.201545Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        out_channels = m.weight.data.shape[0]\n",
    "        if layer_id in skip['A']:\n",
    "            cfg_mask.append(torch.ones(out_channels))\n",
    "            cfg.append(out_channels)\n",
    "            layer_id += 1\n",
    "            continue\n",
    "        if layer_id % 2 == 0:\n",
    "            if layer_id <= 18:\n",
    "                stage = 0\n",
    "            elif layer_id <= 36:\n",
    "                stage = 1\n",
    "            else:\n",
    "                stage = 2\n",
    "            prune_prob_stage = prune_prob['A'][stage]\n",
    "            weight_copy = m.weight.data.abs().clone().cpu().numpy()\n",
    "            L1_norm = np.sum(weight_copy, axis=(1,2,3))\n",
    "            num_keep = int(out_channels * (1 - prune_prob_stage))\n",
    "            arg_max = np.argsort(L1_norm)\n",
    "            arg_max_rev = arg_max[::-1][:num_keep]\n",
    "            mask = torch.zeros(out_channels)\n",
    "            mask[arg_max_rev.tolist()] = 1\n",
    "            cfg_mask.append(mask)\n",
    "            cfg.append(num_keep)\n",
    "            layer_id += 1\n",
    "            continue\n",
    "        layer_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T21:10:07.117017Z",
     "iopub.status.busy": "2024-12-11T21:10:07.116646Z",
     "iopub.status.idle": "2024-12-11T21:10:07.121312Z",
     "shell.execute_reply": "2024-12-11T21:10:07.120281Z",
     "shell.execute_reply.started": "2024-12-11T21:10:07.116984Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "newmodel = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T21:10:09.532257Z",
     "iopub.status.busy": "2024-12-11T21:10:09.531876Z",
     "iopub.status.idle": "2024-12-11T21:10:09.595992Z",
     "shell.execute_reply": "2024-12-11T21:10:09.595263Z",
     "shell.execute_reply.started": "2024-12-11T21:10:09.532221Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "start_mask = torch.ones(3)\n",
    "layer_id_in_cfg = 0\n",
    "conv_count = 1\n",
    "for [m0, m1] in zip(model.modules(), newmodel.modules()):\n",
    "    if isinstance(m0, nn.Conv2d):\n",
    "        if conv_count == 1:\n",
    "            m1.weight.data = m0.weight.data.clone()\n",
    "            conv_count += 1\n",
    "            continue\n",
    "        if conv_count % 2 == 0:\n",
    "            mask = cfg_mask[layer_id_in_cfg]\n",
    "            idx = np.squeeze(np.argwhere(np.asarray(mask.cpu().numpy())))\n",
    "            if idx.size == 1:\n",
    "                idx = np.resize(idx, (1,))\n",
    "            w = m0.weight.data[idx.tolist(), :, :, :].clone()\n",
    "            m1.weight.data = w.clone()\n",
    "            layer_id_in_cfg += 1\n",
    "            conv_count += 1\n",
    "            continue\n",
    "        if conv_count % 2 == 1:\n",
    "            mask = cfg_mask[layer_id_in_cfg-1]\n",
    "            idx = np.squeeze(np.argwhere(np.asarray(mask.cpu().numpy())))\n",
    "            if idx.size == 1:\n",
    "                idx = np.resize(idx, (1,))\n",
    "            w = m0.weight.data[:, idx.tolist(), :, :].clone()\n",
    "            m1.weight.data = w.clone()\n",
    "            conv_count += 1\n",
    "            continue\n",
    "    elif isinstance(m0, nn.BatchNorm2d):\n",
    "        if conv_count % 2 == 1:\n",
    "            mask = cfg_mask[layer_id_in_cfg-1]\n",
    "            idx = np.squeeze(np.argwhere(np.asarray(mask.cpu().numpy())))\n",
    "            if idx.size == 1:\n",
    "                idx = np.resize(idx, (1,))\n",
    "            m1.weight.data = m0.weight.data[idx.tolist()].clone()\n",
    "            m1.bias.data = m0.bias.data[idx.tolist()].clone()\n",
    "            m1.running_mean = m0.running_mean[idx.tolist()].clone()\n",
    "            m1.running_var = m0.running_var[idx.tolist()].clone()\n",
    "            continue\n",
    "        m1.weight.data = m0.weight.data.clone()\n",
    "        m1.bias.data = m0.bias.data.clone()\n",
    "        m1.running_mean = m0.running_mean.clone()\n",
    "        m1.running_var = m0.running_var.clone()\n",
    "    elif isinstance(m0, nn.Linear):\n",
    "        m1.weight.data = m0.weight.data.clone()\n",
    "        m1.bias.data = m0.bias.data.clone()\n",
    "\n",
    "torch.save({'cfg': cfg, 'state_dict': newmodel.state_dict()}, os.path.join(save, 'pruned.pth.tar'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-11T21:10:16.926141Z",
     "iopub.status.busy": "2024-12-11T21:10:16.925196Z",
     "iopub.status.idle": "2024-12-11T21:10:16.934181Z",
     "shell.execute_reply": "2024-12-11T21:10:16.933146Z",
     "shell.execute_reply.started": "2024-12-11T21:10:16.926091Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (fc): Linear(in_features=64, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_parameters = sum([param.nelement() for param in newmodel.parameters()])\n",
    "print(newmodel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy and Number of Parameters for the Pruned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T21:10:22.378804Z",
     "iopub.status.busy": "2024-12-11T21:10:22.377953Z",
     "iopub.status.idle": "2024-12-11T21:10:22.382996Z",
     "shell.execute_reply": "2024-12-11T21:10:22.382081Z",
     "shell.execute_reply.started": "2024-12-11T21:10:22.378760Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 562442\n"
     ]
    }
   ],
   "source": [
    "print(\"number of parameters: \"+str(num_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T21:10:26.510472Z",
     "iopub.status.busy": "2024-12-11T21:10:26.510151Z",
     "iopub.status.idle": "2024-12-11T21:11:31.251440Z",
     "shell.execute_reply": "2024-12-11T21:11:31.250526Z",
     "shell.execute_reply.started": "2024-12-11T21:10:26.510444Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 8.9573, Accuracy: 297/10000 (3.0%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc = test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning after pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T21:11:47.448271Z",
     "iopub.status.busy": "2024-12-11T21:11:47.447571Z",
     "iopub.status.idle": "2024-12-11T21:11:47.452994Z",
     "shell.execute_reply": "2024-12-11T21:11:47.451988Z",
     "shell.execute_reply.started": "2024-12-11T21:11:47.448238Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(newmodel.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T21:11:49.147125Z",
     "iopub.status.busy": "2024-12-11T21:11:49.146647Z",
     "iopub.status.idle": "2024-12-11T21:11:49.156866Z",
     "shell.execute_reply": "2024-12-11T21:11:49.155610Z",
     "shell.execute_reply.started": "2024-12-11T21:11:49.147094Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    newmodel.train()\n",
    "    avg_loss = 0.\n",
    "    train_acc = 0.\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: Compute model output\n",
    "        output = newmodel(data)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        avg_loss = avg_loss + loss.item()\n",
    "        \n",
    "        # Use torch.no_grad() for accuracy computation (no gradients required here)\n",
    "        with torch.no_grad():\n",
    "            pred = output.data.max(1, keepdim=True)[1]  # Get index of max log-probability\n",
    "            train_acc += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "        # Backpropagate the loss and optimize the model\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log progress\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.1f}%)]\\tLoss: {loss.item():.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-11T21:11:49.762685Z",
     "iopub.status.busy": "2024-12-11T21:11:49.762336Z",
     "iopub.status.idle": "2024-12-11T22:47:17.509416Z",
     "shell.execute_reply": "2024-12-11T22:47:17.508448Z",
     "shell.execute_reply.started": "2024-12-11T21:11:49.762653Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/50000 (0.0%)]\tLoss: 2.762380\n",
      "Train Epoch: 0 [6400/50000 (12.8%)]\tLoss: 3.021941\n",
      "Train Epoch: 0 [12800/50000 (25.6%)]\tLoss: 2.303818\n",
      "Train Epoch: 0 [19200/50000 (38.4%)]\tLoss: 2.404951\n",
      "Train Epoch: 0 [25600/50000 (51.2%)]\tLoss: 2.334910\n",
      "Train Epoch: 0 [32000/50000 (63.9%)]\tLoss: 2.590052\n",
      "Train Epoch: 0 [38400/50000 (76.7%)]\tLoss: 2.442172\n",
      "Train Epoch: 0 [44800/50000 (89.5%)]\tLoss: 2.385309\n",
      "\n",
      "Test set: Average loss: 2.4715, Accuracy: 3590/10000 (35.9%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0.0%)]\tLoss: 2.401487\n",
      "Train Epoch: 1 [6400/50000 (12.8%)]\tLoss: 2.424259\n",
      "Train Epoch: 1 [12800/50000 (25.6%)]\tLoss: 2.481336\n",
      "Train Epoch: 1 [19200/50000 (38.4%)]\tLoss: 2.878996\n",
      "Train Epoch: 1 [25600/50000 (51.2%)]\tLoss: 2.409033\n",
      "Train Epoch: 1 [32000/50000 (63.9%)]\tLoss: 2.456629\n",
      "Train Epoch: 1 [38400/50000 (76.7%)]\tLoss: 2.498177\n",
      "Train Epoch: 1 [44800/50000 (89.5%)]\tLoss: 2.518283\n",
      "\n",
      "Test set: Average loss: 2.3801, Accuracy: 3848/10000 (38.5%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0.0%)]\tLoss: 2.602745\n",
      "Train Epoch: 2 [6400/50000 (12.8%)]\tLoss: 2.418850\n",
      "Train Epoch: 2 [12800/50000 (25.6%)]\tLoss: 2.010627\n",
      "Train Epoch: 2 [19200/50000 (38.4%)]\tLoss: 2.183642\n",
      "Train Epoch: 2 [25600/50000 (51.2%)]\tLoss: 2.260052\n",
      "Train Epoch: 2 [32000/50000 (63.9%)]\tLoss: 2.565501\n",
      "Train Epoch: 2 [38400/50000 (76.7%)]\tLoss: 2.581445\n",
      "Train Epoch: 2 [44800/50000 (89.5%)]\tLoss: 2.290493\n",
      "\n",
      "Test set: Average loss: 2.3362, Accuracy: 3867/10000 (38.7%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0.0%)]\tLoss: 2.596474\n",
      "Train Epoch: 3 [6400/50000 (12.8%)]\tLoss: 2.357632\n",
      "Train Epoch: 3 [12800/50000 (25.6%)]\tLoss: 2.429754\n",
      "Train Epoch: 3 [19200/50000 (38.4%)]\tLoss: 2.241588\n",
      "Train Epoch: 3 [25600/50000 (51.2%)]\tLoss: 2.758430\n",
      "Train Epoch: 3 [32000/50000 (63.9%)]\tLoss: 2.653889\n",
      "Train Epoch: 3 [38400/50000 (76.7%)]\tLoss: 2.025117\n",
      "Train Epoch: 3 [44800/50000 (89.5%)]\tLoss: 2.406578\n",
      "\n",
      "Test set: Average loss: 2.3117, Accuracy: 3992/10000 (39.9%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0.0%)]\tLoss: 2.246662\n",
      "Train Epoch: 4 [6400/50000 (12.8%)]\tLoss: 2.381954\n",
      "Train Epoch: 4 [12800/50000 (25.6%)]\tLoss: 2.065155\n",
      "Train Epoch: 4 [19200/50000 (38.4%)]\tLoss: 2.030536\n",
      "Train Epoch: 4 [25600/50000 (51.2%)]\tLoss: 2.277920\n",
      "Train Epoch: 4 [32000/50000 (63.9%)]\tLoss: 1.893297\n",
      "Train Epoch: 4 [38400/50000 (76.7%)]\tLoss: 2.402455\n",
      "Train Epoch: 4 [44800/50000 (89.5%)]\tLoss: 2.246524\n",
      "\n",
      "Test set: Average loss: 2.3242, Accuracy: 3937/10000 (39.4%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0.0%)]\tLoss: 2.079118\n",
      "Train Epoch: 5 [6400/50000 (12.8%)]\tLoss: 1.991718\n",
      "Train Epoch: 5 [12800/50000 (25.6%)]\tLoss: 2.217548\n",
      "Train Epoch: 5 [19200/50000 (38.4%)]\tLoss: 2.049012\n",
      "Train Epoch: 5 [25600/50000 (51.2%)]\tLoss: 2.112208\n",
      "Train Epoch: 5 [32000/50000 (63.9%)]\tLoss: 2.318101\n",
      "Train Epoch: 5 [38400/50000 (76.7%)]\tLoss: 2.179317\n",
      "Train Epoch: 5 [44800/50000 (89.5%)]\tLoss: 2.050238\n",
      "\n",
      "Test set: Average loss: 2.2935, Accuracy: 3992/10000 (39.9%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0.0%)]\tLoss: 2.002474\n",
      "Train Epoch: 6 [6400/50000 (12.8%)]\tLoss: 2.082870\n",
      "Train Epoch: 6 [12800/50000 (25.6%)]\tLoss: 2.474820\n",
      "Train Epoch: 6 [19200/50000 (38.4%)]\tLoss: 2.292944\n",
      "Train Epoch: 6 [25600/50000 (51.2%)]\tLoss: 1.988383\n",
      "Train Epoch: 6 [32000/50000 (63.9%)]\tLoss: 2.336153\n",
      "Train Epoch: 6 [38400/50000 (76.7%)]\tLoss: 1.819961\n",
      "Train Epoch: 6 [44800/50000 (89.5%)]\tLoss: 2.329470\n",
      "\n",
      "Test set: Average loss: 2.2457, Accuracy: 4064/10000 (40.6%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0.0%)]\tLoss: 2.350399\n",
      "Train Epoch: 7 [6400/50000 (12.8%)]\tLoss: 2.075589\n",
      "Train Epoch: 7 [12800/50000 (25.6%)]\tLoss: 2.656211\n",
      "Train Epoch: 7 [19200/50000 (38.4%)]\tLoss: 1.930474\n",
      "Train Epoch: 7 [25600/50000 (51.2%)]\tLoss: 2.203412\n",
      "Train Epoch: 7 [32000/50000 (63.9%)]\tLoss: 2.368882\n",
      "Train Epoch: 7 [38400/50000 (76.7%)]\tLoss: 2.164785\n",
      "Train Epoch: 7 [44800/50000 (89.5%)]\tLoss: 2.209943\n",
      "\n",
      "Test set: Average loss: 2.3059, Accuracy: 4013/10000 (40.1%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0.0%)]\tLoss: 2.239341\n",
      "Train Epoch: 8 [6400/50000 (12.8%)]\tLoss: 2.393171\n",
      "Train Epoch: 8 [12800/50000 (25.6%)]\tLoss: 1.847216\n",
      "Train Epoch: 8 [19200/50000 (38.4%)]\tLoss: 2.168226\n",
      "Train Epoch: 8 [25600/50000 (51.2%)]\tLoss: 1.890102\n",
      "Train Epoch: 8 [32000/50000 (63.9%)]\tLoss: 2.474535\n",
      "Train Epoch: 8 [38400/50000 (76.7%)]\tLoss: 1.944617\n",
      "Train Epoch: 8 [44800/50000 (89.5%)]\tLoss: 2.190881\n",
      "\n",
      "Test set: Average loss: 2.1987, Accuracy: 4212/10000 (42.1%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0.0%)]\tLoss: 2.389604\n",
      "Train Epoch: 9 [6400/50000 (12.8%)]\tLoss: 2.258533\n",
      "Train Epoch: 9 [12800/50000 (25.6%)]\tLoss: 1.792778\n",
      "Train Epoch: 9 [19200/50000 (38.4%)]\tLoss: 2.290991\n",
      "Train Epoch: 9 [25600/50000 (51.2%)]\tLoss: 2.467790\n",
      "Train Epoch: 9 [32000/50000 (63.9%)]\tLoss: 2.013761\n",
      "Train Epoch: 9 [38400/50000 (76.7%)]\tLoss: 1.978831\n",
      "Train Epoch: 9 [44800/50000 (89.5%)]\tLoss: 2.017389\n",
      "\n",
      "Test set: Average loss: 2.1732, Accuracy: 4268/10000 (42.7%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0.0%)]\tLoss: 1.838301\n",
      "Train Epoch: 10 [6400/50000 (12.8%)]\tLoss: 1.623038\n",
      "Train Epoch: 10 [12800/50000 (25.6%)]\tLoss: 1.709901\n",
      "Train Epoch: 10 [19200/50000 (38.4%)]\tLoss: 2.129527\n",
      "Train Epoch: 10 [25600/50000 (51.2%)]\tLoss: 2.069717\n",
      "Train Epoch: 10 [32000/50000 (63.9%)]\tLoss: 2.206302\n",
      "Train Epoch: 10 [38400/50000 (76.7%)]\tLoss: 1.765110\n",
      "Train Epoch: 10 [44800/50000 (89.5%)]\tLoss: 2.271060\n",
      "\n",
      "Test set: Average loss: 2.1448, Accuracy: 4319/10000 (43.2%)\n",
      "\n",
      "Train Epoch: 11 [0/50000 (0.0%)]\tLoss: 2.012175\n",
      "Train Epoch: 11 [6400/50000 (12.8%)]\tLoss: 2.285935\n",
      "Train Epoch: 11 [12800/50000 (25.6%)]\tLoss: 1.878686\n",
      "Train Epoch: 11 [19200/50000 (38.4%)]\tLoss: 1.899618\n",
      "Train Epoch: 11 [25600/50000 (51.2%)]\tLoss: 1.903421\n",
      "Train Epoch: 11 [32000/50000 (63.9%)]\tLoss: 2.156386\n",
      "Train Epoch: 11 [38400/50000 (76.7%)]\tLoss: 1.877692\n",
      "Train Epoch: 11 [44800/50000 (89.5%)]\tLoss: 2.334400\n",
      "\n",
      "Test set: Average loss: 2.1299, Accuracy: 4338/10000 (43.4%)\n",
      "\n",
      "Train Epoch: 12 [0/50000 (0.0%)]\tLoss: 2.139005\n",
      "Train Epoch: 12 [6400/50000 (12.8%)]\tLoss: 2.201729\n",
      "Train Epoch: 12 [12800/50000 (25.6%)]\tLoss: 2.222595\n",
      "Train Epoch: 12 [19200/50000 (38.4%)]\tLoss: 2.331963\n",
      "Train Epoch: 12 [25600/50000 (51.2%)]\tLoss: 1.940211\n",
      "Train Epoch: 12 [32000/50000 (63.9%)]\tLoss: 1.996140\n",
      "Train Epoch: 12 [38400/50000 (76.7%)]\tLoss: 1.623589\n",
      "Train Epoch: 12 [44800/50000 (89.5%)]\tLoss: 2.138823\n",
      "\n",
      "Test set: Average loss: 2.1307, Accuracy: 4365/10000 (43.6%)\n",
      "\n",
      "Train Epoch: 13 [0/50000 (0.0%)]\tLoss: 1.780531\n",
      "Train Epoch: 13 [6400/50000 (12.8%)]\tLoss: 1.821249\n",
      "Train Epoch: 13 [12800/50000 (25.6%)]\tLoss: 2.198189\n",
      "Train Epoch: 13 [19200/50000 (38.4%)]\tLoss: 1.903006\n",
      "Train Epoch: 13 [25600/50000 (51.2%)]\tLoss: 1.900406\n",
      "Train Epoch: 13 [32000/50000 (63.9%)]\tLoss: 1.956738\n",
      "Train Epoch: 13 [38400/50000 (76.7%)]\tLoss: 2.073855\n",
      "Train Epoch: 13 [44800/50000 (89.5%)]\tLoss: 1.926353\n",
      "\n",
      "Test set: Average loss: 2.0975, Accuracy: 4455/10000 (44.5%)\n",
      "\n",
      "Train Epoch: 14 [0/50000 (0.0%)]\tLoss: 1.932303\n",
      "Train Epoch: 14 [6400/50000 (12.8%)]\tLoss: 1.584186\n",
      "Train Epoch: 14 [12800/50000 (25.6%)]\tLoss: 1.922135\n",
      "Train Epoch: 14 [19200/50000 (38.4%)]\tLoss: 1.941466\n",
      "Train Epoch: 14 [25600/50000 (51.2%)]\tLoss: 2.074174\n",
      "Train Epoch: 14 [32000/50000 (63.9%)]\tLoss: 2.012484\n",
      "Train Epoch: 14 [38400/50000 (76.7%)]\tLoss: 1.943402\n",
      "Train Epoch: 14 [44800/50000 (89.5%)]\tLoss: 2.143479\n",
      "\n",
      "Test set: Average loss: 2.1044, Accuracy: 4390/10000 (43.9%)\n",
      "\n",
      "Train Epoch: 15 [0/50000 (0.0%)]\tLoss: 1.850153\n",
      "Train Epoch: 15 [6400/50000 (12.8%)]\tLoss: 1.877896\n",
      "Train Epoch: 15 [12800/50000 (25.6%)]\tLoss: 1.916231\n",
      "Train Epoch: 15 [19200/50000 (38.4%)]\tLoss: 1.620060\n",
      "Train Epoch: 15 [25600/50000 (51.2%)]\tLoss: 2.437435\n",
      "Train Epoch: 15 [32000/50000 (63.9%)]\tLoss: 1.560009\n",
      "Train Epoch: 15 [38400/50000 (76.7%)]\tLoss: 1.870564\n",
      "Train Epoch: 15 [44800/50000 (89.5%)]\tLoss: 1.985279\n",
      "\n",
      "Test set: Average loss: 2.1433, Accuracy: 4403/10000 (44.0%)\n",
      "\n",
      "Train Epoch: 16 [0/50000 (0.0%)]\tLoss: 1.595221\n",
      "Train Epoch: 16 [6400/50000 (12.8%)]\tLoss: 1.883103\n",
      "Train Epoch: 16 [12800/50000 (25.6%)]\tLoss: 1.776194\n",
      "Train Epoch: 16 [19200/50000 (38.4%)]\tLoss: 2.031512\n",
      "Train Epoch: 16 [25600/50000 (51.2%)]\tLoss: 1.484354\n",
      "Train Epoch: 16 [32000/50000 (63.9%)]\tLoss: 2.147996\n",
      "Train Epoch: 16 [38400/50000 (76.7%)]\tLoss: 1.752194\n",
      "Train Epoch: 16 [44800/50000 (89.5%)]\tLoss: 2.251991\n",
      "\n",
      "Test set: Average loss: 2.0570, Accuracy: 4555/10000 (45.5%)\n",
      "\n",
      "Train Epoch: 17 [0/50000 (0.0%)]\tLoss: 2.043935\n",
      "Train Epoch: 17 [6400/50000 (12.8%)]\tLoss: 2.022352\n",
      "Train Epoch: 17 [12800/50000 (25.6%)]\tLoss: 1.985496\n",
      "Train Epoch: 17 [19200/50000 (38.4%)]\tLoss: 2.050362\n",
      "Train Epoch: 17 [25600/50000 (51.2%)]\tLoss: 2.481756\n",
      "Train Epoch: 17 [32000/50000 (63.9%)]\tLoss: 1.743480\n",
      "Train Epoch: 17 [38400/50000 (76.7%)]\tLoss: 1.999489\n",
      "Train Epoch: 17 [44800/50000 (89.5%)]\tLoss: 2.228797\n",
      "\n",
      "Test set: Average loss: 2.0618, Accuracy: 4543/10000 (45.4%)\n",
      "\n",
      "Train Epoch: 18 [0/50000 (0.0%)]\tLoss: 1.815871\n",
      "Train Epoch: 18 [6400/50000 (12.8%)]\tLoss: 2.112279\n",
      "Train Epoch: 18 [12800/50000 (25.6%)]\tLoss: 1.996633\n",
      "Train Epoch: 18 [19200/50000 (38.4%)]\tLoss: 2.185315\n",
      "Train Epoch: 18 [25600/50000 (51.2%)]\tLoss: 2.367614\n",
      "Train Epoch: 18 [32000/50000 (63.9%)]\tLoss: 2.149968\n",
      "Train Epoch: 18 [38400/50000 (76.7%)]\tLoss: 1.839903\n",
      "Train Epoch: 18 [44800/50000 (89.5%)]\tLoss: 1.333245\n",
      "\n",
      "Test set: Average loss: 2.0563, Accuracy: 4549/10000 (45.5%)\n",
      "\n",
      "Train Epoch: 19 [0/50000 (0.0%)]\tLoss: 2.033637\n",
      "Train Epoch: 19 [6400/50000 (12.8%)]\tLoss: 1.752935\n",
      "Train Epoch: 19 [12800/50000 (25.6%)]\tLoss: 1.884002\n",
      "Train Epoch: 19 [19200/50000 (38.4%)]\tLoss: 1.882803\n",
      "Train Epoch: 19 [25600/50000 (51.2%)]\tLoss: 1.874927\n",
      "Train Epoch: 19 [32000/50000 (63.9%)]\tLoss: 1.990145\n",
      "Train Epoch: 19 [38400/50000 (76.7%)]\tLoss: 1.752202\n",
      "Train Epoch: 19 [44800/50000 (89.5%)]\tLoss: 2.214621\n",
      "\n",
      "Test set: Average loss: 2.0238, Accuracy: 4602/10000 (46.0%)\n",
      "\n",
      "Train Epoch: 20 [0/50000 (0.0%)]\tLoss: 2.051529\n",
      "Train Epoch: 20 [6400/50000 (12.8%)]\tLoss: 1.482966\n",
      "Train Epoch: 20 [12800/50000 (25.6%)]\tLoss: 1.858018\n",
      "Train Epoch: 20 [19200/50000 (38.4%)]\tLoss: 1.995971\n",
      "Train Epoch: 20 [25600/50000 (51.2%)]\tLoss: 1.907567\n",
      "Train Epoch: 20 [32000/50000 (63.9%)]\tLoss: 2.130359\n",
      "Train Epoch: 20 [38400/50000 (76.7%)]\tLoss: 1.900950\n",
      "Train Epoch: 20 [44800/50000 (89.5%)]\tLoss: 1.998621\n",
      "\n",
      "Test set: Average loss: 2.0943, Accuracy: 4464/10000 (44.6%)\n",
      "\n",
      "Train Epoch: 21 [0/50000 (0.0%)]\tLoss: 1.645510\n",
      "Train Epoch: 21 [6400/50000 (12.8%)]\tLoss: 1.570316\n",
      "Train Epoch: 21 [12800/50000 (25.6%)]\tLoss: 2.003887\n",
      "Train Epoch: 21 [19200/50000 (38.4%)]\tLoss: 1.653656\n",
      "Train Epoch: 21 [25600/50000 (51.2%)]\tLoss: 2.002320\n",
      "Train Epoch: 21 [32000/50000 (63.9%)]\tLoss: 1.833802\n",
      "Train Epoch: 21 [38400/50000 (76.7%)]\tLoss: 1.744823\n",
      "Train Epoch: 21 [44800/50000 (89.5%)]\tLoss: 1.738222\n",
      "\n",
      "Test set: Average loss: 2.0398, Accuracy: 4589/10000 (45.9%)\n",
      "\n",
      "Train Epoch: 22 [0/50000 (0.0%)]\tLoss: 1.616816\n",
      "Train Epoch: 22 [6400/50000 (12.8%)]\tLoss: 1.666248\n",
      "Train Epoch: 22 [12800/50000 (25.6%)]\tLoss: 1.867052\n",
      "Train Epoch: 22 [19200/50000 (38.4%)]\tLoss: 1.987784\n",
      "Train Epoch: 22 [25600/50000 (51.2%)]\tLoss: 2.089409\n",
      "Train Epoch: 22 [32000/50000 (63.9%)]\tLoss: 2.082394\n",
      "Train Epoch: 22 [38400/50000 (76.7%)]\tLoss: 1.972728\n",
      "Train Epoch: 22 [44800/50000 (89.5%)]\tLoss: 1.591368\n",
      "\n",
      "Test set: Average loss: 1.9509, Accuracy: 4762/10000 (47.6%)\n",
      "\n",
      "Train Epoch: 23 [0/50000 (0.0%)]\tLoss: 1.759798\n",
      "Train Epoch: 23 [6400/50000 (12.8%)]\tLoss: 1.871660\n",
      "Train Epoch: 23 [12800/50000 (25.6%)]\tLoss: 2.101945\n",
      "Train Epoch: 23 [19200/50000 (38.4%)]\tLoss: 1.537971\n",
      "Train Epoch: 23 [25600/50000 (51.2%)]\tLoss: 1.773386\n",
      "Train Epoch: 23 [32000/50000 (63.9%)]\tLoss: 1.840077\n",
      "Train Epoch: 23 [38400/50000 (76.7%)]\tLoss: 1.891917\n",
      "Train Epoch: 23 [44800/50000 (89.5%)]\tLoss: 1.695730\n",
      "\n",
      "Test set: Average loss: 2.0158, Accuracy: 4666/10000 (46.7%)\n",
      "\n",
      "Train Epoch: 24 [0/50000 (0.0%)]\tLoss: 1.923606\n",
      "Train Epoch: 24 [6400/50000 (12.8%)]\tLoss: 1.923630\n",
      "Train Epoch: 24 [12800/50000 (25.6%)]\tLoss: 1.841749\n",
      "Train Epoch: 24 [19200/50000 (38.4%)]\tLoss: 1.763021\n",
      "Train Epoch: 24 [25600/50000 (51.2%)]\tLoss: 1.731029\n",
      "Train Epoch: 24 [32000/50000 (63.9%)]\tLoss: 1.654303\n",
      "Train Epoch: 24 [38400/50000 (76.7%)]\tLoss: 1.919187\n",
      "Train Epoch: 24 [44800/50000 (89.5%)]\tLoss: 1.677231\n",
      "\n",
      "Test set: Average loss: 1.9963, Accuracy: 4698/10000 (47.0%)\n",
      "\n",
      "Train Epoch: 25 [0/50000 (0.0%)]\tLoss: 1.682362\n",
      "Train Epoch: 25 [6400/50000 (12.8%)]\tLoss: 1.772957\n",
      "Train Epoch: 25 [12800/50000 (25.6%)]\tLoss: 1.733015\n",
      "Train Epoch: 25 [19200/50000 (38.4%)]\tLoss: 1.532201\n",
      "Train Epoch: 25 [25600/50000 (51.2%)]\tLoss: 1.686584\n",
      "Train Epoch: 25 [32000/50000 (63.9%)]\tLoss: 1.552002\n",
      "Train Epoch: 25 [38400/50000 (76.7%)]\tLoss: 1.834510\n",
      "Train Epoch: 25 [44800/50000 (89.5%)]\tLoss: 1.955529\n",
      "\n",
      "Test set: Average loss: 1.9840, Accuracy: 4747/10000 (47.5%)\n",
      "\n",
      "Train Epoch: 26 [0/50000 (0.0%)]\tLoss: 1.858709\n",
      "Train Epoch: 26 [6400/50000 (12.8%)]\tLoss: 1.906794\n",
      "Train Epoch: 26 [12800/50000 (25.6%)]\tLoss: 1.557284\n",
      "Train Epoch: 26 [19200/50000 (38.4%)]\tLoss: 1.743312\n",
      "Train Epoch: 26 [25600/50000 (51.2%)]\tLoss: 1.721033\n",
      "Train Epoch: 26 [32000/50000 (63.9%)]\tLoss: 2.093128\n",
      "Train Epoch: 26 [38400/50000 (76.7%)]\tLoss: 1.707065\n",
      "Train Epoch: 26 [44800/50000 (89.5%)]\tLoss: 1.936031\n",
      "\n",
      "Test set: Average loss: 2.0127, Accuracy: 4725/10000 (47.2%)\n",
      "\n",
      "Train Epoch: 27 [0/50000 (0.0%)]\tLoss: 2.026823\n",
      "Train Epoch: 27 [6400/50000 (12.8%)]\tLoss: 1.450363\n",
      "Train Epoch: 27 [12800/50000 (25.6%)]\tLoss: 1.464537\n",
      "Train Epoch: 27 [19200/50000 (38.4%)]\tLoss: 1.777848\n",
      "Train Epoch: 27 [25600/50000 (51.2%)]\tLoss: 1.522435\n",
      "Train Epoch: 27 [32000/50000 (63.9%)]\tLoss: 1.835418\n",
      "Train Epoch: 27 [38400/50000 (76.7%)]\tLoss: 2.048129\n",
      "Train Epoch: 27 [44800/50000 (89.5%)]\tLoss: 1.877598\n",
      "\n",
      "Test set: Average loss: 1.9189, Accuracy: 4841/10000 (48.4%)\n",
      "\n",
      "Train Epoch: 28 [0/50000 (0.0%)]\tLoss: 1.622765\n",
      "Train Epoch: 28 [6400/50000 (12.8%)]\tLoss: 1.821539\n",
      "Train Epoch: 28 [12800/50000 (25.6%)]\tLoss: 1.901299\n",
      "Train Epoch: 28 [19200/50000 (38.4%)]\tLoss: 1.600973\n",
      "Train Epoch: 28 [25600/50000 (51.2%)]\tLoss: 2.018095\n",
      "Train Epoch: 28 [32000/50000 (63.9%)]\tLoss: 1.642401\n",
      "Train Epoch: 28 [38400/50000 (76.7%)]\tLoss: 1.628781\n",
      "Train Epoch: 28 [44800/50000 (89.5%)]\tLoss: 1.618675\n",
      "\n",
      "Test set: Average loss: 1.9500, Accuracy: 4777/10000 (47.8%)\n",
      "\n",
      "Train Epoch: 29 [0/50000 (0.0%)]\tLoss: 1.736086\n",
      "Train Epoch: 29 [6400/50000 (12.8%)]\tLoss: 1.526986\n",
      "Train Epoch: 29 [12800/50000 (25.6%)]\tLoss: 2.030518\n",
      "Train Epoch: 29 [19200/50000 (38.4%)]\tLoss: 2.001084\n",
      "Train Epoch: 29 [25600/50000 (51.2%)]\tLoss: 2.434567\n",
      "Train Epoch: 29 [32000/50000 (63.9%)]\tLoss: 1.859488\n",
      "Train Epoch: 29 [38400/50000 (76.7%)]\tLoss: 1.896519\n",
      "Train Epoch: 29 [44800/50000 (89.5%)]\tLoss: 1.929174\n",
      "\n",
      "Test set: Average loss: 1.8929, Accuracy: 4919/10000 (49.2%)\n",
      "\n",
      "Train Epoch: 30 [0/50000 (0.0%)]\tLoss: 1.879033\n",
      "Train Epoch: 30 [6400/50000 (12.8%)]\tLoss: 2.084509\n",
      "Train Epoch: 30 [12800/50000 (25.6%)]\tLoss: 1.568447\n",
      "Train Epoch: 30 [19200/50000 (38.4%)]\tLoss: 1.707685\n",
      "Train Epoch: 30 [25600/50000 (51.2%)]\tLoss: 1.639339\n",
      "Train Epoch: 30 [32000/50000 (63.9%)]\tLoss: 1.614123\n",
      "Train Epoch: 30 [38400/50000 (76.7%)]\tLoss: 2.003040\n",
      "Train Epoch: 30 [44800/50000 (89.5%)]\tLoss: 1.672239\n",
      "\n",
      "Test set: Average loss: 1.9207, Accuracy: 4882/10000 (48.8%)\n",
      "\n",
      "Train Epoch: 31 [0/50000 (0.0%)]\tLoss: 1.552705\n",
      "Train Epoch: 31 [6400/50000 (12.8%)]\tLoss: 1.611947\n",
      "Train Epoch: 31 [12800/50000 (25.6%)]\tLoss: 1.395651\n",
      "Train Epoch: 31 [19200/50000 (38.4%)]\tLoss: 1.642794\n",
      "Train Epoch: 31 [25600/50000 (51.2%)]\tLoss: 1.434844\n",
      "Train Epoch: 31 [32000/50000 (63.9%)]\tLoss: 1.421208\n",
      "Train Epoch: 31 [38400/50000 (76.7%)]\tLoss: 2.075399\n",
      "Train Epoch: 31 [44800/50000 (89.5%)]\tLoss: 1.713390\n",
      "\n",
      "Test set: Average loss: 1.9040, Accuracy: 4876/10000 (48.8%)\n",
      "\n",
      "Train Epoch: 32 [0/50000 (0.0%)]\tLoss: 1.995232\n",
      "Train Epoch: 32 [6400/50000 (12.8%)]\tLoss: 1.722491\n",
      "Train Epoch: 32 [12800/50000 (25.6%)]\tLoss: 2.140049\n",
      "Train Epoch: 32 [19200/50000 (38.4%)]\tLoss: 1.711057\n",
      "Train Epoch: 32 [25600/50000 (51.2%)]\tLoss: 1.718102\n",
      "Train Epoch: 32 [32000/50000 (63.9%)]\tLoss: 1.426576\n",
      "Train Epoch: 32 [38400/50000 (76.7%)]\tLoss: 1.539152\n",
      "Train Epoch: 32 [44800/50000 (89.5%)]\tLoss: 1.747072\n",
      "\n",
      "Test set: Average loss: 1.8736, Accuracy: 5013/10000 (50.1%)\n",
      "\n",
      "Train Epoch: 33 [0/50000 (0.0%)]\tLoss: 1.579119\n",
      "Train Epoch: 33 [6400/50000 (12.8%)]\tLoss: 1.775382\n",
      "Train Epoch: 33 [12800/50000 (25.6%)]\tLoss: 1.670797\n",
      "Train Epoch: 33 [19200/50000 (38.4%)]\tLoss: 1.907321\n",
      "Train Epoch: 33 [25600/50000 (51.2%)]\tLoss: 1.857271\n",
      "Train Epoch: 33 [32000/50000 (63.9%)]\tLoss: 1.501140\n",
      "Train Epoch: 33 [38400/50000 (76.7%)]\tLoss: 1.471384\n",
      "Train Epoch: 33 [44800/50000 (89.5%)]\tLoss: 1.744186\n",
      "\n",
      "Test set: Average loss: 1.9329, Accuracy: 4839/10000 (48.4%)\n",
      "\n",
      "Train Epoch: 34 [0/50000 (0.0%)]\tLoss: 1.611696\n",
      "Train Epoch: 34 [6400/50000 (12.8%)]\tLoss: 1.445713\n",
      "Train Epoch: 34 [12800/50000 (25.6%)]\tLoss: 1.957617\n",
      "Train Epoch: 34 [19200/50000 (38.4%)]\tLoss: 1.778063\n",
      "Train Epoch: 34 [25600/50000 (51.2%)]\tLoss: 1.654873\n",
      "Train Epoch: 34 [32000/50000 (63.9%)]\tLoss: 1.477973\n",
      "Train Epoch: 34 [38400/50000 (76.7%)]\tLoss: 1.838695\n",
      "Train Epoch: 34 [44800/50000 (89.5%)]\tLoss: 1.886648\n",
      "\n",
      "Test set: Average loss: 1.8588, Accuracy: 5018/10000 (50.2%)\n",
      "\n",
      "Train Epoch: 35 [0/50000 (0.0%)]\tLoss: 1.487238\n",
      "Train Epoch: 35 [6400/50000 (12.8%)]\tLoss: 1.904693\n",
      "Train Epoch: 35 [12800/50000 (25.6%)]\tLoss: 1.763137\n",
      "Train Epoch: 35 [19200/50000 (38.4%)]\tLoss: 1.736257\n",
      "Train Epoch: 35 [25600/50000 (51.2%)]\tLoss: 1.327496\n",
      "Train Epoch: 35 [32000/50000 (63.9%)]\tLoss: 1.362285\n",
      "Train Epoch: 35 [38400/50000 (76.7%)]\tLoss: 1.466318\n",
      "Train Epoch: 35 [44800/50000 (89.5%)]\tLoss: 1.709372\n",
      "\n",
      "Test set: Average loss: 1.8489, Accuracy: 5011/10000 (50.1%)\n",
      "\n",
      "Train Epoch: 36 [0/50000 (0.0%)]\tLoss: 1.790004\n",
      "Train Epoch: 36 [6400/50000 (12.8%)]\tLoss: 1.684387\n",
      "Train Epoch: 36 [12800/50000 (25.6%)]\tLoss: 1.864825\n",
      "Train Epoch: 36 [19200/50000 (38.4%)]\tLoss: 1.504269\n",
      "Train Epoch: 36 [25600/50000 (51.2%)]\tLoss: 1.834719\n",
      "Train Epoch: 36 [32000/50000 (63.9%)]\tLoss: 1.267467\n",
      "Train Epoch: 36 [38400/50000 (76.7%)]\tLoss: 1.655610\n",
      "Train Epoch: 36 [44800/50000 (89.5%)]\tLoss: 1.653724\n",
      "\n",
      "Test set: Average loss: 1.9214, Accuracy: 4892/10000 (48.9%)\n",
      "\n",
      "Train Epoch: 37 [0/50000 (0.0%)]\tLoss: 1.700727\n",
      "Train Epoch: 37 [6400/50000 (12.8%)]\tLoss: 1.885236\n",
      "Train Epoch: 37 [12800/50000 (25.6%)]\tLoss: 1.834720\n",
      "Train Epoch: 37 [19200/50000 (38.4%)]\tLoss: 1.794637\n",
      "Train Epoch: 37 [25600/50000 (51.2%)]\tLoss: 1.502710\n",
      "Train Epoch: 37 [32000/50000 (63.9%)]\tLoss: 1.873432\n",
      "Train Epoch: 37 [38400/50000 (76.7%)]\tLoss: 1.506690\n",
      "Train Epoch: 37 [44800/50000 (89.5%)]\tLoss: 1.584777\n",
      "\n",
      "Test set: Average loss: 1.9232, Accuracy: 4902/10000 (49.0%)\n",
      "\n",
      "Train Epoch: 38 [0/50000 (0.0%)]\tLoss: 1.543750\n",
      "Train Epoch: 38 [6400/50000 (12.8%)]\tLoss: 1.619701\n",
      "Train Epoch: 38 [12800/50000 (25.6%)]\tLoss: 1.563499\n",
      "Train Epoch: 38 [19200/50000 (38.4%)]\tLoss: 1.579396\n",
      "Train Epoch: 38 [25600/50000 (51.2%)]\tLoss: 2.178547\n",
      "Train Epoch: 38 [32000/50000 (63.9%)]\tLoss: 1.882628\n",
      "Train Epoch: 38 [38400/50000 (76.7%)]\tLoss: 1.598202\n",
      "Train Epoch: 38 [44800/50000 (89.5%)]\tLoss: 1.331216\n",
      "\n",
      "Test set: Average loss: 1.8649, Accuracy: 5001/10000 (50.0%)\n",
      "\n",
      "Train Epoch: 39 [0/50000 (0.0%)]\tLoss: 1.726396\n",
      "Train Epoch: 39 [6400/50000 (12.8%)]\tLoss: 1.894940\n",
      "Train Epoch: 39 [12800/50000 (25.6%)]\tLoss: 1.266162\n",
      "Train Epoch: 39 [19200/50000 (38.4%)]\tLoss: 1.811996\n",
      "Train Epoch: 39 [25600/50000 (51.2%)]\tLoss: 1.728122\n",
      "Train Epoch: 39 [32000/50000 (63.9%)]\tLoss: 1.921585\n",
      "Train Epoch: 39 [38400/50000 (76.7%)]\tLoss: 1.481685\n",
      "Train Epoch: 39 [44800/50000 (89.5%)]\tLoss: 1.514594\n",
      "\n",
      "Test set: Average loss: 1.8919, Accuracy: 4977/10000 (49.8%)\n",
      "\n",
      "Train Epoch: 40 [0/50000 (0.0%)]\tLoss: 1.763649\n",
      "Train Epoch: 40 [6400/50000 (12.8%)]\tLoss: 1.285594\n",
      "Train Epoch: 40 [12800/50000 (25.6%)]\tLoss: 1.358864\n",
      "Train Epoch: 40 [19200/50000 (38.4%)]\tLoss: 1.707600\n",
      "Train Epoch: 40 [25600/50000 (51.2%)]\tLoss: 1.637717\n",
      "Train Epoch: 40 [32000/50000 (63.9%)]\tLoss: 1.483040\n",
      "Train Epoch: 40 [38400/50000 (76.7%)]\tLoss: 1.398572\n",
      "Train Epoch: 40 [44800/50000 (89.5%)]\tLoss: 1.895882\n",
      "\n",
      "Test set: Average loss: 1.8795, Accuracy: 5040/10000 (50.4%)\n",
      "\n",
      "Train Epoch: 41 [0/50000 (0.0%)]\tLoss: 1.397282\n",
      "Train Epoch: 41 [6400/50000 (12.8%)]\tLoss: 1.453939\n",
      "Train Epoch: 41 [12800/50000 (25.6%)]\tLoss: 1.254135\n",
      "Train Epoch: 41 [19200/50000 (38.4%)]\tLoss: 1.671073\n",
      "Train Epoch: 41 [25600/50000 (51.2%)]\tLoss: 1.374161\n",
      "Train Epoch: 41 [32000/50000 (63.9%)]\tLoss: 1.697659\n",
      "Train Epoch: 41 [38400/50000 (76.7%)]\tLoss: 1.840698\n",
      "Train Epoch: 41 [44800/50000 (89.5%)]\tLoss: 1.250485\n",
      "\n",
      "Test set: Average loss: 1.8277, Accuracy: 5125/10000 (51.2%)\n",
      "\n",
      "Train Epoch: 42 [0/50000 (0.0%)]\tLoss: 1.581731\n",
      "Train Epoch: 42 [6400/50000 (12.8%)]\tLoss: 1.675404\n",
      "Train Epoch: 42 [12800/50000 (25.6%)]\tLoss: 1.395205\n",
      "Train Epoch: 42 [19200/50000 (38.4%)]\tLoss: 1.814135\n",
      "Train Epoch: 42 [25600/50000 (51.2%)]\tLoss: 1.574527\n",
      "Train Epoch: 42 [32000/50000 (63.9%)]\tLoss: 1.920881\n",
      "Train Epoch: 42 [38400/50000 (76.7%)]\tLoss: 1.774277\n",
      "Train Epoch: 42 [44800/50000 (89.5%)]\tLoss: 1.898922\n",
      "\n",
      "Test set: Average loss: 1.9137, Accuracy: 5013/10000 (50.1%)\n",
      "\n",
      "Train Epoch: 43 [0/50000 (0.0%)]\tLoss: 1.237647\n",
      "Train Epoch: 43 [6400/50000 (12.8%)]\tLoss: 1.257995\n",
      "Train Epoch: 43 [12800/50000 (25.6%)]\tLoss: 1.534800\n",
      "Train Epoch: 43 [19200/50000 (38.4%)]\tLoss: 1.709910\n",
      "Train Epoch: 43 [25600/50000 (51.2%)]\tLoss: 1.571455\n",
      "Train Epoch: 43 [32000/50000 (63.9%)]\tLoss: 1.939328\n",
      "Train Epoch: 43 [38400/50000 (76.7%)]\tLoss: 1.356244\n",
      "Train Epoch: 43 [44800/50000 (89.5%)]\tLoss: 2.188570\n",
      "\n",
      "Test set: Average loss: 1.8545, Accuracy: 5099/10000 (51.0%)\n",
      "\n",
      "Train Epoch: 44 [0/50000 (0.0%)]\tLoss: 1.724272\n",
      "Train Epoch: 44 [6400/50000 (12.8%)]\tLoss: 1.349290\n",
      "Train Epoch: 44 [12800/50000 (25.6%)]\tLoss: 1.254609\n",
      "Train Epoch: 44 [19200/50000 (38.4%)]\tLoss: 1.479918\n",
      "Train Epoch: 44 [25600/50000 (51.2%)]\tLoss: 1.572306\n",
      "Train Epoch: 44 [32000/50000 (63.9%)]\tLoss: 1.542455\n",
      "Train Epoch: 44 [38400/50000 (76.7%)]\tLoss: 1.837361\n",
      "Train Epoch: 44 [44800/50000 (89.5%)]\tLoss: 1.541748\n",
      "\n",
      "Test set: Average loss: 1.8462, Accuracy: 5145/10000 (51.5%)\n",
      "\n",
      "Train Epoch: 45 [0/50000 (0.0%)]\tLoss: 1.102618\n",
      "Train Epoch: 45 [6400/50000 (12.8%)]\tLoss: 1.580012\n",
      "Train Epoch: 45 [12800/50000 (25.6%)]\tLoss: 1.548337\n",
      "Train Epoch: 45 [19200/50000 (38.4%)]\tLoss: 1.466951\n",
      "Train Epoch: 45 [25600/50000 (51.2%)]\tLoss: 1.800127\n",
      "Train Epoch: 45 [32000/50000 (63.9%)]\tLoss: 1.598493\n",
      "Train Epoch: 45 [38400/50000 (76.7%)]\tLoss: 1.481418\n",
      "Train Epoch: 45 [44800/50000 (89.5%)]\tLoss: 1.704053\n",
      "\n",
      "Test set: Average loss: 1.8459, Accuracy: 5147/10000 (51.5%)\n",
      "\n",
      "Train Epoch: 46 [0/50000 (0.0%)]\tLoss: 1.596719\n",
      "Train Epoch: 46 [6400/50000 (12.8%)]\tLoss: 1.704187\n",
      "Train Epoch: 46 [12800/50000 (25.6%)]\tLoss: 1.601073\n",
      "Train Epoch: 46 [19200/50000 (38.4%)]\tLoss: 1.189275\n",
      "Train Epoch: 46 [25600/50000 (51.2%)]\tLoss: 1.785605\n",
      "Train Epoch: 46 [32000/50000 (63.9%)]\tLoss: 1.367971\n",
      "Train Epoch: 46 [38400/50000 (76.7%)]\tLoss: 1.367321\n",
      "Train Epoch: 46 [44800/50000 (89.5%)]\tLoss: 1.462697\n",
      "\n",
      "Test set: Average loss: 1.8445, Accuracy: 5114/10000 (51.1%)\n",
      "\n",
      "Train Epoch: 47 [0/50000 (0.0%)]\tLoss: 1.457544\n",
      "Train Epoch: 47 [6400/50000 (12.8%)]\tLoss: 1.848929\n",
      "Train Epoch: 47 [12800/50000 (25.6%)]\tLoss: 1.549034\n",
      "Train Epoch: 47 [19200/50000 (38.4%)]\tLoss: 1.443766\n",
      "Train Epoch: 47 [25600/50000 (51.2%)]\tLoss: 1.408668\n",
      "Train Epoch: 47 [32000/50000 (63.9%)]\tLoss: 1.539561\n",
      "Train Epoch: 47 [38400/50000 (76.7%)]\tLoss: 1.486726\n",
      "Train Epoch: 47 [44800/50000 (89.5%)]\tLoss: 1.716786\n",
      "\n",
      "Test set: Average loss: 1.8370, Accuracy: 5105/10000 (51.0%)\n",
      "\n",
      "Train Epoch: 48 [0/50000 (0.0%)]\tLoss: 1.796267\n",
      "Train Epoch: 48 [6400/50000 (12.8%)]\tLoss: 1.983083\n",
      "Train Epoch: 48 [12800/50000 (25.6%)]\tLoss: 1.266068\n",
      "Train Epoch: 48 [19200/50000 (38.4%)]\tLoss: 1.916574\n",
      "Train Epoch: 48 [25600/50000 (51.2%)]\tLoss: 1.651792\n",
      "Train Epoch: 48 [32000/50000 (63.9%)]\tLoss: 1.910738\n",
      "Train Epoch: 48 [38400/50000 (76.7%)]\tLoss: 1.502057\n",
      "Train Epoch: 48 [44800/50000 (89.5%)]\tLoss: 1.875050\n",
      "\n",
      "Test set: Average loss: 1.8296, Accuracy: 5169/10000 (51.7%)\n",
      "\n",
      "Train Epoch: 49 [0/50000 (0.0%)]\tLoss: 1.481880\n",
      "Train Epoch: 49 [6400/50000 (12.8%)]\tLoss: 1.414868\n",
      "Train Epoch: 49 [12800/50000 (25.6%)]\tLoss: 1.401509\n",
      "Train Epoch: 49 [19200/50000 (38.4%)]\tLoss: 1.591858\n",
      "Train Epoch: 49 [25600/50000 (51.2%)]\tLoss: 1.693015\n",
      "Train Epoch: 49 [32000/50000 (63.9%)]\tLoss: 1.263748\n",
      "Train Epoch: 49 [38400/50000 (76.7%)]\tLoss: 1.649465\n",
      "Train Epoch: 49 [44800/50000 (89.5%)]\tLoss: 1.306400\n",
      "\n",
      "Test set: Average loss: 1.7871, Accuracy: 5236/10000 (52.4%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_prec1 = 0.\n",
    "epochs = 50\n",
    "newmodel.train()\n",
    "newmodel.to(device)\n",
    "for epoch in range(0, epochs):\n",
    "    train(epoch)\n",
    "    prec1 = test()\n",
    "    is_best = prec1 > best_prec1\n",
    "    best_prec1 = max(prec1, best_prec1)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': newmodel.state_dict(),\n",
    "        'best_prec1': best_prec1,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'cfg': newmodel.cfg\n",
    "    }, is_best, filepath=save)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy and Number of Parameters for the Finetuned Pruned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T22:48:43.449276Z",
     "iopub.status.busy": "2024-12-11T22:48:43.448993Z",
     "iopub.status.idle": "2024-12-11T22:49:57.804554Z",
     "shell.execute_reply": "2024-12-11T22:49:57.803639Z",
     "shell.execute_reply.started": "2024-12-11T22:48:43.449249Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the Baseline model\n",
      "\n",
      "Test set: Average loss: 1.7871, Accuracy: 5236/10000 (52.4%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newmodel.eval()\n",
    "print('Accuracy for the Baseline model')\n",
    "acc = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T22:49:57.805999Z",
     "iopub.status.busy": "2024-12-11T22:49:57.805735Z",
     "iopub.status.idle": "2024-12-11T22:49:57.811016Z",
     "shell.execute_reply": "2024-12-11T22:49:57.810176Z",
     "shell.execute_reply.started": "2024-12-11T22:49:57.805974Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562442\n"
     ]
    }
   ],
   "source": [
    "num_parameters = sum([param.nelement() for param in newmodel.parameters()])\n",
    "print(num_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T22:49:57.812451Z",
     "iopub.status.busy": "2024-12-11T22:49:57.812203Z",
     "iopub.status.idle": "2024-12-11T22:50:15.211327Z",
     "shell.execute_reply": "2024-12-11T22:50:15.210442Z",
     "shell.execute_reply.started": "2024-12-11T22:49:57.812424Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference         0.00%       0.000us         0.00%       0.000us       0.000us     424.108ms        51.28%     424.108ms      21.205ms            20  \n",
      "                                        model_inference         3.09%      53.850ms        22.89%     399.329ms      39.933ms       0.000us         0.00%     397.623ms      39.762ms            10  \n",
      "                                           aten::conv2d         0.13%       2.297ms         2.69%      46.918ms      95.751us       0.000us         0.00%     279.207ms     569.811us           490  \n",
      "                                      aten::convolution         0.33%       5.832ms         2.56%      44.621ms      91.064us       0.000us         0.00%     279.207ms     569.811us           490  \n",
      "                                     aten::_convolution         0.21%       3.697ms         2.22%      38.790ms      79.162us       0.000us         0.00%     279.207ms     569.811us           490  \n",
      "                                aten::cudnn_convolution         1.32%      23.078ms         2.01%      35.093ms      71.617us     279.207ms        33.76%     279.207ms     569.811us           490  \n",
      "       cudnn_infer_volta_scudnn_128x32_relu_small_nn_v1         0.00%       0.000us         0.00%       0.000us       0.000us     215.357ms        26.04%     215.357ms     652.597us           330  \n",
      "                                       aten::batch_norm         0.07%       1.270ms         2.30%      40.101ms      81.839us       0.000us         0.00%      43.247ms      88.259us           490  \n",
      "                           aten::_batch_norm_impl_index         0.12%       2.152ms         2.23%      38.831ms      79.247us       0.000us         0.00%      43.247ms      88.259us           490  \n",
      "                                 aten::cudnn_batch_norm         0.96%      16.709ms         2.10%      36.679ms      74.856us      43.247ms         5.23%      43.247ms      88.259us           490  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.744s\n",
      "Self CUDA time total: 827.089ms\n",
      "\n",
      "\n",
      "Latency per batch: 0.0400 seconds\n",
      "Throughput: 6392.3477 samples/second\n",
      "Peak GPU memory usage: 1906.67 MB\n",
      "Model parameter count: 562442\n",
      "FLOPs estimation: 1.12e+06 FLOPs\n"
     ]
    }
   ],
   "source": [
    "profile_model(newmodel, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
