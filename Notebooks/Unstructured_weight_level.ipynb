{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T23:55:01.129440Z",
     "iopub.status.busy": "2024-12-11T23:55:01.129174Z",
     "iopub.status.idle": "2024-12-11T23:55:01.140810Z",
     "shell.execute_reply": "2024-12-11T23:55:01.140109Z",
     "shell.execute_reply.started": "2024-12-11T23:55:01.129403Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os, sys\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T23:55:01.142147Z",
     "iopub.status.busy": "2024-12-11T23:55:01.141780Z",
     "iopub.status.idle": "2024-12-11T23:55:01.149085Z",
     "shell.execute_reply": "2024-12-11T23:55:01.148188Z",
     "shell.execute_reply.started": "2024-12-11T23:55:01.142105Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = 'cifar100'\n",
    "depth = 50\n",
    "batch_size = 64\n",
    "test_batch_size = 256\n",
    "epochs = 50\n",
    "lr = 0.001\n",
    "log_interval = 100\n",
    "save = '/kaggle/working/'\n",
    "arch = 'resnet'\n",
    "seed = 1\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T23:55:01.150854Z",
     "iopub.status.busy": "2024-12-11T23:55:01.150216Z",
     "iopub.status.idle": "2024-12-11T23:55:01.162799Z",
     "shell.execute_reply": "2024-12-11T23:55:01.162121Z",
     "shell.execute_reply.started": "2024-12-11T23:55:01.150795Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import errno\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "__all__ = ['get_mean_and_std', 'init_params', 'mkdir_p', 'AverageMeter']\n",
    "\n",
    "def get_mean_and_std(dataset):\n",
    "    '''Compute the mean and std value of dataset.'''\n",
    "    dataloader = trainloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    print('==> Computing mean and std..')\n",
    "    for inputs, targets in dataloader:\n",
    "        for i in range(3):\n",
    "            mean[i] += inputs[:,i,:,:].mean()\n",
    "            std[i] += inputs[:,i,:,:].std()\n",
    "    mean.div_(len(dataset))\n",
    "    std.div_(len(dataset))\n",
    "    return mean, std\n",
    "\n",
    "def get_conv_zero_param(model):\n",
    "    total = 0\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            total += torch.sum(m.weight.data.eq(0))\n",
    "    return total\n",
    "\n",
    "def init_params(net):\n",
    "    '''Init layer parameters.'''\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            init.kaiming_normal(m.weight, mode='fan_out')\n",
    "            if m.bias:\n",
    "                init.constant(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            init.constant(m.weight, 1)\n",
    "            init.constant(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            init.normal(m.weight, std=1e-3)\n",
    "            if m.bias:\n",
    "                init.constant(m.bias, 0)\n",
    "\n",
    "def mkdir_p(path):\n",
    "    '''make dir if not exist'''\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc:  # Python >2.5\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\n",
    "       Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T23:55:03.799486Z",
     "iopub.status.busy": "2024-12-11T23:55:03.798806Z",
     "iopub.status.idle": "2024-12-11T23:55:05.373791Z",
     "shell.execute_reply": "2024-12-11T23:55:05.372902Z",
     "shell.execute_reply.started": "2024-12-11T23:55:03.799451Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR100('./data.cifar100', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.Pad(4),\n",
    "                           transforms.RandomCrop(32),\n",
    "                           transforms.RandomHorizontalFlip(),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR100('./data.cifar100', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T23:55:05.375951Z",
     "iopub.status.busy": "2024-12-11T23:55:05.375587Z",
     "iopub.status.idle": "2024-12-11T23:55:05.430256Z",
     "shell.execute_reply": "2024-12-11T23:55:05.429083Z",
     "shell.execute_reply.started": "2024-12-11T23:55:05.375916Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "__all__ = ['resnet']\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, cfg, stride=1, downsample=None):\n",
    "        # cfg should be a number in this case\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, cfg, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(cfg)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(cfg, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "def downsample_basic_block(x, planes):\n",
    "    x = nn.AvgPool2d(2,2)(x)\n",
    "    zero_pads = torch.Tensor(\n",
    "        x.size(0), planes - x.size(1), x.size(2), x.size(3)).zero_()\n",
    "    if isinstance(x.data, torch.cuda.FloatTensor):\n",
    "        zero_pads = zero_pads.cuda()\n",
    "\n",
    "    out = Variable(torch.cat([x.data, zero_pads], dim=1))\n",
    "\n",
    "    return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, depth, dataset='cifar10', cfg=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        # Model type specifies number of layers for CIFAR-10 model\n",
    "        assert (depth - 2) % 6 == 0, 'depth should be 6n+2'\n",
    "        n = (depth - 2) // 6\n",
    "\n",
    "        block = BasicBlock\n",
    "        if cfg == None:\n",
    "            cfg = [[16]*n, [32]*n, [64]*n]\n",
    "            cfg = [item for sub_list in cfg for item in sub_list]\n",
    "\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.inplanes = 16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 16, n, cfg=cfg[0:n])\n",
    "        self.layer2 = self._make_layer(block, 32, n, cfg=cfg[n:2*n], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, n, cfg=cfg[2*n:3*n], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(8)\n",
    "        if dataset == 'cifar10':\n",
    "            num_classes = 10\n",
    "        elif dataset == 'cifar100':\n",
    "            num_classes = 100\n",
    "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, cfg, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = partial(downsample_basic_block, planes=planes*block.expansion)\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, cfg[0], stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, cfg[i]))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)    # 32x32\n",
    "\n",
    "        x = self.layer1(x)  # 32x32\n",
    "        x = self.layer2(x)  # 16x16\n",
    "        x = self.layer3(x)  # 8x8\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def resnet(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a ResNet model.\n",
    "    \"\"\"\n",
    "    return ResNet(**kwargs)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    net = resnet(depth=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T23:55:05.933145Z",
     "iopub.status.busy": "2024-12-11T23:55:05.932264Z",
     "iopub.status.idle": "2024-12-11T23:55:05.964734Z",
     "shell.execute_reply": "2024-12-11T23:55:05.963909Z",
     "shell.execute_reply.started": "2024-12-11T23:55:05.933104Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = resnet(dataset=dataset, depth=depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T23:55:57.813204Z",
     "iopub.status.busy": "2024-12-11T23:55:57.812766Z",
     "iopub.status.idle": "2024-12-11T23:55:57.818415Z",
     "shell.execute_reply": "2024-12-11T23:55:57.816942Z",
     "shell.execute_reply.started": "2024-12-11T23:55:57.813166Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T23:55:58.272956Z",
     "iopub.status.busy": "2024-12-11T23:55:58.272148Z",
     "iopub.status.idle": "2024-12-11T23:55:58.377150Z",
     "shell.execute_reply": "2024-12-11T23:55:58.376145Z",
     "shell.execute_reply.started": "2024-12-11T23:55:58.272919Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (6): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (7): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (6): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (7): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (6): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (7): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "  (fc): Linear(in_features=64, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('/kaggle/input/base_resnet50/pytorch/default/1/resnet_model.pth', weights_only = True))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T23:55:59.203409Z",
     "iopub.status.busy": "2024-12-11T23:55:59.202564Z",
     "iopub.status.idle": "2024-12-11T23:55:59.210212Z",
     "shell.execute_reply": "2024-12-11T23:55:59.209268Z",
     "shell.execute_reply.started": "2024-12-11T23:55:59.203370Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.profiler import profile, ProfilerActivity\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    # Start profiling\n",
    "    with profile(\n",
    "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True,\n",
    "    ) as prof:  # Profiler object initialized here\n",
    "        with torch.no_grad():  # Use torch.no_grad() for inference\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                output = model(data)\n",
    "                test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum batch loss\n",
    "                pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    prof.export_chrome_trace(\"trace.json\")\n",
    "\n",
    "    # Calculate test loss and accuracy\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "    return correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T23:55:59.934262Z",
     "iopub.status.busy": "2024-12-11T23:55:59.933459Z",
     "iopub.status.idle": "2024-12-11T23:55:59.939204Z",
     "shell.execute_reply": "2024-12-11T23:55:59.938264Z",
     "shell.execute_reply.started": "2024-12-11T23:55:59.934227Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T23:57:29.109098Z",
     "iopub.status.busy": "2024-12-11T23:57:29.108049Z",
     "iopub.status.idle": "2024-12-11T23:57:29.120653Z",
     "shell.execute_reply": "2024-12-11T23:57:29.119356Z",
     "shell.execute_reply.started": "2024-12-11T23:57:29.109040Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "import torch.quantization as quant\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import time\n",
    "\n",
    "def profile_model(model, data_loader, num_batches=10):\n",
    "    model.eval()\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "\n",
    "    with profile(\n",
    "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./logs'),\n",
    "        record_shapes=True,\n",
    "        with_stack=True\n",
    "    ) as prof:\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            if batch_idx >= num_batches:\n",
    "                break\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            start_time = time.time()\n",
    "            with record_function(\"model_inference\"):\n",
    "                outputs = model(inputs)\n",
    "            latency = time.time() - start_time\n",
    "\n",
    "    print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "    print(f\"\\nLatency per batch: {latency:.4f} seconds\")\n",
    "    print(f\"Throughput: {len(inputs) / latency:.4f} samples/second\")\n",
    "    print(f\"Peak GPU memory usage: {torch.cuda.max_memory_allocated(device) / (1024 * 1024):.2f} MB\")\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model parameter count: {total_params}\")\n",
    "    print(f\"FLOPs estimation: {total_params * 2:.2e} FLOPs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T23:57:29.123328Z",
     "iopub.status.busy": "2024-12-11T23:57:29.122266Z",
     "iopub.status.idle": "2024-12-11T23:57:34.784932Z",
     "shell.execute_reply": "2024-12-11T23:57:34.783985Z",
     "shell.execute_reply.started": "2024-12-11T23:57:29.123283Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference         0.00%       0.000us         0.00%       0.000us       0.000us     197.622ms        61.39%     197.622ms      19.762ms            10  \n",
      "                                        model_inference        11.32%      58.563ms        39.65%     205.148ms      20.515ms       0.000us         0.00%     123.634ms      12.363ms            10  \n",
      "                                           aten::conv2d         0.41%       2.122ms        10.08%      52.168ms     106.465us       0.000us         0.00%      91.780ms     187.305us           490  \n",
      "                                      aten::convolution         1.06%       5.482ms         9.67%      50.045ms     102.133us       0.000us         0.00%      91.780ms     187.305us           490  \n",
      "                                     aten::_convolution         0.76%       3.908ms         8.61%      44.563ms      90.945us       0.000us         0.00%      91.780ms     187.305us           490  \n",
      "                                aten::cudnn_convolution         4.51%      23.332ms         7.86%      40.656ms      82.970us      91.780ms        28.51%      91.780ms     187.305us           490  \n",
      "       cudnn_infer_volta_scudnn_128x32_relu_small_nn_v1         0.00%       0.000us         0.00%       0.000us       0.000us      74.306ms        23.08%      74.306ms     225.168us           330  \n",
      "                                       aten::batch_norm         0.28%       1.450ms         9.74%      50.389ms     102.834us       0.000us         0.00%      13.238ms      27.015us           490  \n",
      "                           aten::_batch_norm_impl_index         0.48%       2.460ms         9.46%      48.939ms      99.875us       0.000us         0.00%      13.238ms      27.015us           490  \n",
      "                                 aten::cudnn_batch_norm         3.50%      18.117ms         8.98%      46.479ms      94.855us      13.238ms         4.11%      13.238ms      27.015us           490  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 517.440ms\n",
      "Self CUDA time total: 321.930ms\n",
      "\n",
      "\n",
      "Latency per batch: 0.0182 seconds\n",
      "Throughput: 3510.1991 samples/second\n",
      "Peak GPU memory usage: 480.74 MB\n",
      "Model parameter count: 761652\n",
      "FLOPs estimation: 1.52e+06 FLOPs\n"
     ]
    }
   ],
   "source": [
    "profile_model(model,test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T23:57:34.788544Z",
     "iopub.status.busy": "2024-12-11T23:57:34.787891Z",
     "iopub.status.idle": "2024-12-11T23:57:34.922693Z",
     "shell.execute_reply": "2024-12-11T23:57:34.921645Z",
     "shell.execute_reply.started": "2024-12-11T23:57:34.788499Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning threshold: 0.054346729069948196\n",
      "layer index: 1 \t total params: 432 \t remaining params: 274\n",
      "layer index: 6 \t total params: 2304 \t remaining params: 1526\n",
      "layer index: 9 \t total params: 2304 \t remaining params: 1465\n",
      "layer index: 12 \t total params: 2304 \t remaining params: 1530\n",
      "layer index: 15 \t total params: 2304 \t remaining params: 1503\n",
      "layer index: 18 \t total params: 2304 \t remaining params: 1488\n",
      "layer index: 21 \t total params: 2304 \t remaining params: 1508\n",
      "layer index: 24 \t total params: 2304 \t remaining params: 1474\n",
      "layer index: 27 \t total params: 2304 \t remaining params: 1479\n",
      "layer index: 30 \t total params: 2304 \t remaining params: 1468\n",
      "layer index: 33 \t total params: 2304 \t remaining params: 1494\n",
      "layer index: 36 \t total params: 2304 \t remaining params: 1479\n",
      "layer index: 39 \t total params: 2304 \t remaining params: 1501\n",
      "layer index: 42 \t total params: 2304 \t remaining params: 1467\n",
      "layer index: 45 \t total params: 2304 \t remaining params: 1486\n",
      "layer index: 48 \t total params: 2304 \t remaining params: 1435\n",
      "layer index: 51 \t total params: 2304 \t remaining params: 1479\n",
      "layer index: 55 \t total params: 4608 \t remaining params: 2364\n",
      "layer index: 58 \t total params: 9216 \t remaining params: 4731\n",
      "layer index: 61 \t total params: 9216 \t remaining params: 4699\n",
      "layer index: 64 \t total params: 9216 \t remaining params: 4743\n",
      "layer index: 67 \t total params: 9216 \t remaining params: 4706\n",
      "layer index: 70 \t total params: 9216 \t remaining params: 4733\n",
      "layer index: 73 \t total params: 9216 \t remaining params: 4714\n",
      "layer index: 76 \t total params: 9216 \t remaining params: 4675\n",
      "layer index: 79 \t total params: 9216 \t remaining params: 4770\n",
      "layer index: 82 \t total params: 9216 \t remaining params: 4763\n",
      "layer index: 85 \t total params: 9216 \t remaining params: 4773\n",
      "layer index: 88 \t total params: 9216 \t remaining params: 4738\n",
      "layer index: 91 \t total params: 9216 \t remaining params: 4761\n",
      "layer index: 94 \t total params: 9216 \t remaining params: 4729\n",
      "layer index: 97 \t total params: 9216 \t remaining params: 4767\n",
      "layer index: 100 \t total params: 9216 \t remaining params: 4758\n",
      "layer index: 104 \t total params: 18432 \t remaining params: 6438\n",
      "layer index: 107 \t total params: 36864 \t remaining params: 13202\n",
      "layer index: 110 \t total params: 36864 \t remaining params: 13070\n",
      "layer index: 113 \t total params: 36864 \t remaining params: 13217\n",
      "layer index: 116 \t total params: 36864 \t remaining params: 13177\n",
      "layer index: 119 \t total params: 36864 \t remaining params: 12957\n",
      "layer index: 122 \t total params: 36864 \t remaining params: 13052\n",
      "layer index: 125 \t total params: 36864 \t remaining params: 13114\n",
      "layer index: 128 \t total params: 36864 \t remaining params: 12942\n",
      "layer index: 131 \t total params: 36864 \t remaining params: 13164\n",
      "layer index: 134 \t total params: 36864 \t remaining params: 13165\n",
      "layer index: 137 \t total params: 36864 \t remaining params: 13087\n",
      "layer index: 140 \t total params: 36864 \t remaining params: 13042\n",
      "layer index: 143 \t total params: 36864 \t remaining params: 13207\n",
      "layer index: 146 \t total params: 36864 \t remaining params: 13112\n",
      "layer index: 149 \t total params: 36864 \t remaining params: 13188\n",
      "Total conv params: 751536, Pruned conv params: 450922.0, Pruned ratio: 0.6000005602836609\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "percent = 0.6\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        total += m.weight.data.numel()\n",
    "conv_weights = torch.zeros(total)\n",
    "index = 0\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        size = m.weight.data.numel()\n",
    "        conv_weights[index:(index+size)] = m.weight.data.view(-1).abs().clone()\n",
    "        index += size\n",
    "\n",
    "y, i = torch.sort(conv_weights)\n",
    "thre_index = int(total * percent)\n",
    "thre = y[thre_index]\n",
    "pruned = 0\n",
    "print('Pruning threshold: {}'.format(thre))\n",
    "zero_flag = False\n",
    "for k, m in enumerate(model.modules()):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        weight_copy = m.weight.data.abs().clone()\n",
    "        mask = weight_copy.gt(thre).float().cuda()\n",
    "        pruned = pruned + mask.numel() - torch.sum(mask)\n",
    "        m.weight.data.mul_(mask)\n",
    "        if int(torch.sum(mask)) == 0:\n",
    "            zero_flag = True\n",
    "        print('layer index: {:d} \\t total params: {:d} \\t remaining params: {:d}'.\n",
    "            format(k, mask.numel(), int(torch.sum(mask))))\n",
    "print('Total conv params: {}, Pruned conv params: {}, Pruned ratio: {}'.format(total, pruned, pruned/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T23:57:34.924308Z",
     "iopub.status.busy": "2024-12-11T23:57:34.923980Z",
     "iopub.status.idle": "2024-12-11T23:59:08.238782Z",
     "shell.execute_reply": "2024-12-11T23:59:08.237892Z",
     "shell.execute_reply.started": "2024-12-11T23:57:34.924276Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after pruning\n",
      "\n",
      "Test set: Average loss: 563.7897, Accuracy: 100/10000 (1.0%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy after pruning')\n",
    "acc = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T23:59:08.240694Z",
     "iopub.status.busy": "2024-12-11T23:59:08.240232Z",
     "iopub.status.idle": "2024-12-11T23:59:08.246597Z",
     "shell.execute_reply": "2024-12-11T23:59:08.245532Z",
     "shell.execute_reply.started": "2024-12-11T23:59:08.240645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filepath):\n",
    "    torch.save(state, os.path.join(filepath, 'checkpoint.pth.tar'))\n",
    "    if is_best:\n",
    "        shutil.copyfile(os.path.join(filepath, 'checkpoint.pth.tar'), os.path.join(filepath, 'model_best.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T23:59:08.248126Z",
     "iopub.status.busy": "2024-12-11T23:59:08.247800Z",
     "iopub.status.idle": "2024-12-11T23:59:08.285773Z",
     "shell.execute_reply": "2024-12-11T23:59:08.284973Z",
     "shell.execute_reply.started": "2024-12-11T23:59:08.248096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "save_checkpoint({\n",
    "        'epoch': 0,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec1': 0,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'cfg': model.cfg\n",
    "    }, is_best=0, filepath=save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T23:59:08.287319Z",
     "iopub.status.busy": "2024-12-11T23:59:08.287063Z",
     "iopub.status.idle": "2024-12-11T23:59:08.292134Z",
     "shell.execute_reply": "2024-12-11T23:59:08.291290Z",
     "shell.execute_reply.started": "2024-12-11T23:59:08.287293Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T00:00:33.461556Z",
     "iopub.status.busy": "2024-12-12T00:00:33.460641Z",
     "iopub.status.idle": "2024-12-12T00:00:33.469163Z",
     "shell.execute_reply": "2024-12-12T00:00:33.467914Z",
     "shell.execute_reply.started": "2024-12-12T00:00:33.461509Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "    train_acc = 0.\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: Compute model output\n",
    "        output = model(data)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        avg_loss = avg_loss + loss.item()\n",
    "        \n",
    "        # Use torch.no_grad() for accuracy computation (no gradients required here)\n",
    "        with torch.no_grad():\n",
    "            pred = output.data.max(1, keepdim=True)[1]  # Get index of max log-probability\n",
    "            train_acc += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "        # Backpropagate the loss and optimize the model\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log progress\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.1f}%)]\\tLoss: {loss.item():.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T00:23:26.248261Z",
     "iopub.status.busy": "2024-12-12T00:23:26.247890Z",
     "iopub.status.idle": "2024-12-12T02:18:08.870829Z",
     "shell.execute_reply": "2024-12-12T02:18:08.869800Z",
     "shell.execute_reply.started": "2024-12-12T00:23:26.248230Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/50000 (0.0%)]\tLoss: 4.222653\n",
      "Train Epoch: 0 [6400/50000 (12.8%)]\tLoss: 4.264643\n",
      "Train Epoch: 0 [12800/50000 (25.6%)]\tLoss: 3.921251\n",
      "Train Epoch: 0 [19200/50000 (38.4%)]\tLoss: 4.018778\n",
      "Train Epoch: 0 [25600/50000 (51.2%)]\tLoss: 3.974927\n",
      "Train Epoch: 0 [32000/50000 (63.9%)]\tLoss: 4.261743\n",
      "Train Epoch: 0 [38400/50000 (76.7%)]\tLoss: 3.906213\n",
      "Train Epoch: 0 [44800/50000 (89.5%)]\tLoss: 4.304071\n",
      "\n",
      "Test set: Average loss: 3.9906, Accuracy: 855/10000 (8.6%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0.0%)]\tLoss: 4.060011\n",
      "Train Epoch: 1 [6400/50000 (12.8%)]\tLoss: 4.115766\n",
      "Train Epoch: 1 [12800/50000 (25.6%)]\tLoss: 4.055876\n",
      "Train Epoch: 1 [19200/50000 (38.4%)]\tLoss: 3.882263\n",
      "Train Epoch: 1 [25600/50000 (51.2%)]\tLoss: 3.703868\n",
      "Train Epoch: 1 [32000/50000 (63.9%)]\tLoss: 3.753406\n",
      "Train Epoch: 1 [38400/50000 (76.7%)]\tLoss: 3.960434\n",
      "Train Epoch: 1 [44800/50000 (89.5%)]\tLoss: 3.770662\n",
      "\n",
      "Test set: Average loss: 3.8529, Accuracy: 1044/10000 (10.4%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0.0%)]\tLoss: 3.966790\n",
      "Train Epoch: 2 [6400/50000 (12.8%)]\tLoss: 3.822884\n",
      "Train Epoch: 2 [12800/50000 (25.6%)]\tLoss: 3.760896\n",
      "Train Epoch: 2 [19200/50000 (38.4%)]\tLoss: 3.753397\n",
      "Train Epoch: 2 [25600/50000 (51.2%)]\tLoss: 3.903742\n",
      "Train Epoch: 2 [32000/50000 (63.9%)]\tLoss: 3.617362\n",
      "Train Epoch: 2 [38400/50000 (76.7%)]\tLoss: 4.030782\n",
      "Train Epoch: 2 [44800/50000 (89.5%)]\tLoss: 4.082132\n",
      "\n",
      "Test set: Average loss: 3.7762, Accuracy: 1210/10000 (12.1%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0.0%)]\tLoss: 3.592369\n",
      "Train Epoch: 3 [6400/50000 (12.8%)]\tLoss: 3.665193\n",
      "Train Epoch: 3 [12800/50000 (25.6%)]\tLoss: 3.643086\n",
      "Train Epoch: 3 [19200/50000 (38.4%)]\tLoss: 3.678543\n",
      "Train Epoch: 3 [25600/50000 (51.2%)]\tLoss: 3.830272\n",
      "Train Epoch: 3 [32000/50000 (63.9%)]\tLoss: 3.974263\n",
      "Train Epoch: 3 [38400/50000 (76.7%)]\tLoss: 3.524581\n",
      "Train Epoch: 3 [44800/50000 (89.5%)]\tLoss: 3.494441\n",
      "\n",
      "Test set: Average loss: 3.6693, Accuracy: 1315/10000 (13.2%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0.0%)]\tLoss: 4.019014\n",
      "Train Epoch: 4 [6400/50000 (12.8%)]\tLoss: 3.799356\n",
      "Train Epoch: 4 [12800/50000 (25.6%)]\tLoss: 3.568129\n",
      "Train Epoch: 4 [19200/50000 (38.4%)]\tLoss: 3.438513\n",
      "Train Epoch: 4 [25600/50000 (51.2%)]\tLoss: 3.636567\n",
      "Train Epoch: 4 [32000/50000 (63.9%)]\tLoss: 3.381545\n",
      "Train Epoch: 4 [38400/50000 (76.7%)]\tLoss: 3.665506\n",
      "Train Epoch: 4 [44800/50000 (89.5%)]\tLoss: 3.431417\n",
      "\n",
      "Test set: Average loss: 3.5733, Accuracy: 1487/10000 (14.9%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0.0%)]\tLoss: 3.564151\n",
      "Train Epoch: 5 [6400/50000 (12.8%)]\tLoss: 3.447682\n",
      "Train Epoch: 5 [12800/50000 (25.6%)]\tLoss: 3.827778\n",
      "Train Epoch: 5 [19200/50000 (38.4%)]\tLoss: 3.354243\n",
      "Train Epoch: 5 [25600/50000 (51.2%)]\tLoss: 3.621951\n",
      "Train Epoch: 5 [32000/50000 (63.9%)]\tLoss: 3.210307\n",
      "Train Epoch: 5 [38400/50000 (76.7%)]\tLoss: 3.611320\n",
      "Train Epoch: 5 [44800/50000 (89.5%)]\tLoss: 3.553622\n",
      "\n",
      "Test set: Average loss: 3.4863, Accuracy: 1661/10000 (16.6%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0.0%)]\tLoss: 3.332133\n",
      "Train Epoch: 6 [6400/50000 (12.8%)]\tLoss: 3.624811\n",
      "Train Epoch: 6 [12800/50000 (25.6%)]\tLoss: 3.586335\n",
      "Train Epoch: 6 [19200/50000 (38.4%)]\tLoss: 3.388393\n",
      "Train Epoch: 6 [25600/50000 (51.2%)]\tLoss: 3.494054\n",
      "Train Epoch: 6 [32000/50000 (63.9%)]\tLoss: 3.164239\n",
      "Train Epoch: 6 [38400/50000 (76.7%)]\tLoss: 3.345142\n",
      "Train Epoch: 6 [44800/50000 (89.5%)]\tLoss: 3.410231\n",
      "\n",
      "Test set: Average loss: 3.3734, Accuracy: 1858/10000 (18.6%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0.0%)]\tLoss: 3.306920\n",
      "Train Epoch: 7 [6400/50000 (12.8%)]\tLoss: 3.563954\n",
      "Train Epoch: 7 [12800/50000 (25.6%)]\tLoss: 3.526428\n",
      "Train Epoch: 7 [19200/50000 (38.4%)]\tLoss: 3.285096\n",
      "Train Epoch: 7 [25600/50000 (51.2%)]\tLoss: 3.524565\n",
      "Train Epoch: 7 [32000/50000 (63.9%)]\tLoss: 3.592044\n",
      "Train Epoch: 7 [38400/50000 (76.7%)]\tLoss: 3.263492\n",
      "Train Epoch: 7 [44800/50000 (89.5%)]\tLoss: 3.176473\n",
      "\n",
      "Test set: Average loss: 3.3192, Accuracy: 1966/10000 (19.7%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0.0%)]\tLoss: 3.532841\n",
      "Train Epoch: 8 [6400/50000 (12.8%)]\tLoss: 3.146758\n",
      "Train Epoch: 8 [12800/50000 (25.6%)]\tLoss: 3.442153\n",
      "Train Epoch: 8 [19200/50000 (38.4%)]\tLoss: 3.249350\n",
      "Train Epoch: 8 [25600/50000 (51.2%)]\tLoss: 3.296915\n",
      "Train Epoch: 8 [32000/50000 (63.9%)]\tLoss: 3.579745\n",
      "Train Epoch: 8 [38400/50000 (76.7%)]\tLoss: 3.513347\n",
      "Train Epoch: 8 [44800/50000 (89.5%)]\tLoss: 3.286571\n",
      "\n",
      "Test set: Average loss: 3.2883, Accuracy: 1979/10000 (19.8%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0.0%)]\tLoss: 3.415064\n",
      "Train Epoch: 9 [6400/50000 (12.8%)]\tLoss: 3.359483\n",
      "Train Epoch: 9 [12800/50000 (25.6%)]\tLoss: 3.316525\n",
      "Train Epoch: 9 [19200/50000 (38.4%)]\tLoss: 3.337957\n",
      "Train Epoch: 9 [25600/50000 (51.2%)]\tLoss: 3.372547\n",
      "Train Epoch: 9 [32000/50000 (63.9%)]\tLoss: 3.353151\n",
      "Train Epoch: 9 [38400/50000 (76.7%)]\tLoss: 3.341262\n",
      "Train Epoch: 9 [44800/50000 (89.5%)]\tLoss: 2.965235\n",
      "\n",
      "Test set: Average loss: 3.1777, Accuracy: 2225/10000 (22.2%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0.0%)]\tLoss: 3.040654\n",
      "Train Epoch: 10 [6400/50000 (12.8%)]\tLoss: 3.209291\n",
      "Train Epoch: 10 [12800/50000 (25.6%)]\tLoss: 3.148707\n",
      "Train Epoch: 10 [19200/50000 (38.4%)]\tLoss: 3.228699\n",
      "Train Epoch: 10 [25600/50000 (51.2%)]\tLoss: 3.256058\n",
      "Train Epoch: 10 [32000/50000 (63.9%)]\tLoss: 3.020677\n",
      "Train Epoch: 10 [38400/50000 (76.7%)]\tLoss: 3.184448\n",
      "Train Epoch: 10 [44800/50000 (89.5%)]\tLoss: 2.862392\n",
      "\n",
      "Test set: Average loss: 3.1573, Accuracy: 2269/10000 (22.7%)\n",
      "\n",
      "Train Epoch: 11 [0/50000 (0.0%)]\tLoss: 2.961396\n",
      "Train Epoch: 11 [6400/50000 (12.8%)]\tLoss: 3.179350\n",
      "Train Epoch: 11 [12800/50000 (25.6%)]\tLoss: 2.975335\n",
      "Train Epoch: 11 [19200/50000 (38.4%)]\tLoss: 3.215169\n",
      "Train Epoch: 11 [25600/50000 (51.2%)]\tLoss: 2.919450\n",
      "Train Epoch: 11 [32000/50000 (63.9%)]\tLoss: 3.200636\n",
      "Train Epoch: 11 [38400/50000 (76.7%)]\tLoss: 3.345302\n",
      "Train Epoch: 11 [44800/50000 (89.5%)]\tLoss: 3.138281\n",
      "\n",
      "Test set: Average loss: 3.1200, Accuracy: 2316/10000 (23.2%)\n",
      "\n",
      "Train Epoch: 12 [0/50000 (0.0%)]\tLoss: 2.935425\n",
      "Train Epoch: 12 [6400/50000 (12.8%)]\tLoss: 3.416418\n",
      "Train Epoch: 12 [12800/50000 (25.6%)]\tLoss: 2.910799\n",
      "Train Epoch: 12 [19200/50000 (38.4%)]\tLoss: 2.893651\n",
      "Train Epoch: 12 [25600/50000 (51.2%)]\tLoss: 3.650711\n",
      "Train Epoch: 12 [32000/50000 (63.9%)]\tLoss: 2.922246\n",
      "Train Epoch: 12 [38400/50000 (76.7%)]\tLoss: 3.015975\n",
      "Train Epoch: 12 [44800/50000 (89.5%)]\tLoss: 3.041009\n",
      "\n",
      "Test set: Average loss: 3.0561, Accuracy: 2403/10000 (24.0%)\n",
      "\n",
      "Train Epoch: 13 [0/50000 (0.0%)]\tLoss: 3.061556\n",
      "Train Epoch: 13 [6400/50000 (12.8%)]\tLoss: 2.884729\n",
      "Train Epoch: 13 [12800/50000 (25.6%)]\tLoss: 2.952541\n",
      "Train Epoch: 13 [19200/50000 (38.4%)]\tLoss: 3.186900\n",
      "Train Epoch: 13 [25600/50000 (51.2%)]\tLoss: 3.154076\n",
      "Train Epoch: 13 [32000/50000 (63.9%)]\tLoss: 2.748616\n",
      "Train Epoch: 13 [38400/50000 (76.7%)]\tLoss: 3.040821\n",
      "Train Epoch: 13 [44800/50000 (89.5%)]\tLoss: 3.046602\n",
      "\n",
      "Test set: Average loss: 2.9425, Accuracy: 2683/10000 (26.8%)\n",
      "\n",
      "Train Epoch: 14 [0/50000 (0.0%)]\tLoss: 2.904290\n",
      "Train Epoch: 14 [6400/50000 (12.8%)]\tLoss: 2.817280\n",
      "Train Epoch: 14 [12800/50000 (25.6%)]\tLoss: 2.901155\n",
      "Train Epoch: 14 [19200/50000 (38.4%)]\tLoss: 3.016249\n",
      "Train Epoch: 14 [25600/50000 (51.2%)]\tLoss: 3.081427\n",
      "Train Epoch: 14 [32000/50000 (63.9%)]\tLoss: 3.143707\n",
      "Train Epoch: 14 [38400/50000 (76.7%)]\tLoss: 2.847796\n",
      "Train Epoch: 14 [44800/50000 (89.5%)]\tLoss: 2.833694\n",
      "\n",
      "Test set: Average loss: 2.8798, Accuracy: 2755/10000 (27.6%)\n",
      "\n",
      "Train Epoch: 15 [0/50000 (0.0%)]\tLoss: 2.834588\n",
      "Train Epoch: 15 [6400/50000 (12.8%)]\tLoss: 2.781647\n",
      "Train Epoch: 15 [12800/50000 (25.6%)]\tLoss: 2.940784\n",
      "Train Epoch: 15 [19200/50000 (38.4%)]\tLoss: 3.115316\n",
      "Train Epoch: 15 [25600/50000 (51.2%)]\tLoss: 2.400294\n",
      "Train Epoch: 15 [32000/50000 (63.9%)]\tLoss: 3.162187\n",
      "Train Epoch: 15 [38400/50000 (76.7%)]\tLoss: 3.252665\n",
      "Train Epoch: 15 [44800/50000 (89.5%)]\tLoss: 3.208852\n",
      "\n",
      "Test set: Average loss: 2.8508, Accuracy: 2845/10000 (28.4%)\n",
      "\n",
      "Train Epoch: 16 [0/50000 (0.0%)]\tLoss: 2.891263\n",
      "Train Epoch: 16 [6400/50000 (12.8%)]\tLoss: 2.880402\n",
      "Train Epoch: 16 [12800/50000 (25.6%)]\tLoss: 2.794528\n",
      "Train Epoch: 16 [19200/50000 (38.4%)]\tLoss: 2.938477\n",
      "Train Epoch: 16 [25600/50000 (51.2%)]\tLoss: 2.802201\n",
      "Train Epoch: 16 [32000/50000 (63.9%)]\tLoss: 2.801479\n",
      "Train Epoch: 16 [38400/50000 (76.7%)]\tLoss: 2.935535\n",
      "Train Epoch: 16 [44800/50000 (89.5%)]\tLoss: 2.576181\n",
      "\n",
      "Test set: Average loss: 2.8382, Accuracy: 2854/10000 (28.5%)\n",
      "\n",
      "Train Epoch: 17 [0/50000 (0.0%)]\tLoss: 2.635819\n",
      "Train Epoch: 17 [6400/50000 (12.8%)]\tLoss: 2.805347\n",
      "Train Epoch: 17 [12800/50000 (25.6%)]\tLoss: 2.985785\n",
      "Train Epoch: 17 [19200/50000 (38.4%)]\tLoss: 3.010314\n",
      "Train Epoch: 17 [25600/50000 (51.2%)]\tLoss: 2.619297\n",
      "Train Epoch: 17 [32000/50000 (63.9%)]\tLoss: 2.958960\n",
      "Train Epoch: 17 [38400/50000 (76.7%)]\tLoss: 2.921638\n",
      "Train Epoch: 17 [44800/50000 (89.5%)]\tLoss: 2.616695\n",
      "\n",
      "Test set: Average loss: 2.7608, Accuracy: 3020/10000 (30.2%)\n",
      "\n",
      "Train Epoch: 18 [0/50000 (0.0%)]\tLoss: 2.502546\n",
      "Train Epoch: 18 [6400/50000 (12.8%)]\tLoss: 2.997758\n",
      "Train Epoch: 18 [12800/50000 (25.6%)]\tLoss: 3.017004\n",
      "Train Epoch: 18 [19200/50000 (38.4%)]\tLoss: 2.639872\n",
      "Train Epoch: 18 [25600/50000 (51.2%)]\tLoss: 2.723585\n",
      "Train Epoch: 18 [32000/50000 (63.9%)]\tLoss: 2.761651\n",
      "Train Epoch: 18 [38400/50000 (76.7%)]\tLoss: 2.488594\n",
      "Train Epoch: 18 [44800/50000 (89.5%)]\tLoss: 2.923868\n",
      "\n",
      "Test set: Average loss: 2.7266, Accuracy: 3122/10000 (31.2%)\n",
      "\n",
      "Train Epoch: 19 [0/50000 (0.0%)]\tLoss: 2.306502\n",
      "Train Epoch: 19 [6400/50000 (12.8%)]\tLoss: 2.285709\n",
      "Train Epoch: 19 [12800/50000 (25.6%)]\tLoss: 2.471269\n",
      "Train Epoch: 19 [19200/50000 (38.4%)]\tLoss: 2.806231\n",
      "Train Epoch: 19 [25600/50000 (51.2%)]\tLoss: 2.861843\n",
      "Train Epoch: 19 [32000/50000 (63.9%)]\tLoss: 2.579368\n",
      "Train Epoch: 19 [38400/50000 (76.7%)]\tLoss: 2.848549\n",
      "Train Epoch: 19 [44800/50000 (89.5%)]\tLoss: 2.561272\n",
      "\n",
      "Test set: Average loss: 2.6459, Accuracy: 3228/10000 (32.3%)\n",
      "\n",
      "Train Epoch: 20 [0/50000 (0.0%)]\tLoss: 2.433739\n",
      "Train Epoch: 20 [6400/50000 (12.8%)]\tLoss: 2.661535\n",
      "Train Epoch: 20 [12800/50000 (25.6%)]\tLoss: 2.643384\n",
      "Train Epoch: 20 [19200/50000 (38.4%)]\tLoss: 2.456315\n",
      "Train Epoch: 20 [25600/50000 (51.2%)]\tLoss: 2.594423\n",
      "Train Epoch: 20 [32000/50000 (63.9%)]\tLoss: 2.733049\n",
      "Train Epoch: 20 [38400/50000 (76.7%)]\tLoss: 2.705364\n",
      "Train Epoch: 20 [44800/50000 (89.5%)]\tLoss: 2.286959\n",
      "\n",
      "Test set: Average loss: 2.6576, Accuracy: 3249/10000 (32.5%)\n",
      "\n",
      "Train Epoch: 21 [0/50000 (0.0%)]\tLoss: 3.042050\n",
      "Train Epoch: 21 [6400/50000 (12.8%)]\tLoss: 2.875590\n",
      "Train Epoch: 21 [12800/50000 (25.6%)]\tLoss: 2.681071\n",
      "Train Epoch: 21 [19200/50000 (38.4%)]\tLoss: 2.825249\n",
      "Train Epoch: 21 [25600/50000 (51.2%)]\tLoss: 2.353723\n",
      "Train Epoch: 21 [32000/50000 (63.9%)]\tLoss: 2.690949\n",
      "Train Epoch: 21 [38400/50000 (76.7%)]\tLoss: 2.606879\n",
      "Train Epoch: 21 [44800/50000 (89.5%)]\tLoss: 2.232208\n",
      "\n",
      "Test set: Average loss: 2.6321, Accuracy: 3285/10000 (32.9%)\n",
      "\n",
      "Train Epoch: 22 [0/50000 (0.0%)]\tLoss: 2.434456\n",
      "Train Epoch: 22 [6400/50000 (12.8%)]\tLoss: 2.376988\n",
      "Train Epoch: 22 [12800/50000 (25.6%)]\tLoss: 2.327163\n",
      "Train Epoch: 22 [19200/50000 (38.4%)]\tLoss: 2.251251\n",
      "Train Epoch: 22 [25600/50000 (51.2%)]\tLoss: 2.512127\n",
      "Train Epoch: 22 [32000/50000 (63.9%)]\tLoss: 2.383167\n",
      "Train Epoch: 22 [38400/50000 (76.7%)]\tLoss: 2.739542\n",
      "Train Epoch: 22 [44800/50000 (89.5%)]\tLoss: 2.910725\n",
      "\n",
      "Test set: Average loss: 2.6357, Accuracy: 3311/10000 (33.1%)\n",
      "\n",
      "Train Epoch: 23 [0/50000 (0.0%)]\tLoss: 2.411842\n",
      "Train Epoch: 23 [6400/50000 (12.8%)]\tLoss: 2.741334\n",
      "Train Epoch: 23 [12800/50000 (25.6%)]\tLoss: 2.414837\n",
      "Train Epoch: 23 [19200/50000 (38.4%)]\tLoss: 2.589783\n",
      "Train Epoch: 23 [25600/50000 (51.2%)]\tLoss: 2.470047\n",
      "Train Epoch: 23 [32000/50000 (63.9%)]\tLoss: 2.477865\n",
      "Train Epoch: 23 [38400/50000 (76.7%)]\tLoss: 2.290229\n",
      "Train Epoch: 23 [44800/50000 (89.5%)]\tLoss: 2.191685\n",
      "\n",
      "Test set: Average loss: 2.5359, Accuracy: 3463/10000 (34.6%)\n",
      "\n",
      "Train Epoch: 24 [0/50000 (0.0%)]\tLoss: 2.729047\n",
      "Train Epoch: 24 [6400/50000 (12.8%)]\tLoss: 2.559762\n",
      "Train Epoch: 24 [12800/50000 (25.6%)]\tLoss: 2.070997\n",
      "Train Epoch: 24 [19200/50000 (38.4%)]\tLoss: 2.120870\n",
      "Train Epoch: 24 [25600/50000 (51.2%)]\tLoss: 2.541492\n",
      "Train Epoch: 24 [32000/50000 (63.9%)]\tLoss: 2.549198\n",
      "Train Epoch: 24 [38400/50000 (76.7%)]\tLoss: 2.513528\n",
      "Train Epoch: 24 [44800/50000 (89.5%)]\tLoss: 2.526897\n",
      "\n",
      "Test set: Average loss: 2.4800, Accuracy: 3613/10000 (36.1%)\n",
      "\n",
      "Train Epoch: 25 [0/50000 (0.0%)]\tLoss: 2.742258\n",
      "Train Epoch: 25 [6400/50000 (12.8%)]\tLoss: 2.413719\n",
      "Train Epoch: 25 [12800/50000 (25.6%)]\tLoss: 2.741112\n",
      "Train Epoch: 25 [19200/50000 (38.4%)]\tLoss: 2.582225\n",
      "Train Epoch: 25 [25600/50000 (51.2%)]\tLoss: 2.425012\n",
      "Train Epoch: 25 [32000/50000 (63.9%)]\tLoss: 2.970693\n",
      "Train Epoch: 25 [38400/50000 (76.7%)]\tLoss: 2.489534\n",
      "Train Epoch: 25 [44800/50000 (89.5%)]\tLoss: 2.309795\n",
      "\n",
      "Test set: Average loss: 2.5012, Accuracy: 3542/10000 (35.4%)\n",
      "\n",
      "Train Epoch: 26 [0/50000 (0.0%)]\tLoss: 2.129008\n",
      "Train Epoch: 26 [6400/50000 (12.8%)]\tLoss: 2.617806\n",
      "Train Epoch: 26 [12800/50000 (25.6%)]\tLoss: 2.274162\n",
      "Train Epoch: 26 [19200/50000 (38.4%)]\tLoss: 2.511923\n",
      "Train Epoch: 26 [25600/50000 (51.2%)]\tLoss: 2.009359\n",
      "Train Epoch: 26 [32000/50000 (63.9%)]\tLoss: 2.487003\n",
      "Train Epoch: 26 [38400/50000 (76.7%)]\tLoss: 2.490991\n",
      "Train Epoch: 26 [44800/50000 (89.5%)]\tLoss: 2.250709\n",
      "\n",
      "Test set: Average loss: 2.4624, Accuracy: 3663/10000 (36.6%)\n",
      "\n",
      "Train Epoch: 27 [0/50000 (0.0%)]\tLoss: 2.579583\n",
      "Train Epoch: 27 [6400/50000 (12.8%)]\tLoss: 2.461329\n",
      "Train Epoch: 27 [12800/50000 (25.6%)]\tLoss: 2.680866\n",
      "Train Epoch: 27 [19200/50000 (38.4%)]\tLoss: 2.289325\n",
      "Train Epoch: 27 [25600/50000 (51.2%)]\tLoss: 2.529293\n",
      "Train Epoch: 27 [32000/50000 (63.9%)]\tLoss: 2.675766\n",
      "Train Epoch: 27 [38400/50000 (76.7%)]\tLoss: 1.978854\n",
      "Train Epoch: 27 [44800/50000 (89.5%)]\tLoss: 2.355866\n",
      "\n",
      "Test set: Average loss: 2.4256, Accuracy: 3756/10000 (37.6%)\n",
      "\n",
      "Train Epoch: 28 [0/50000 (0.0%)]\tLoss: 2.608918\n",
      "Train Epoch: 28 [6400/50000 (12.8%)]\tLoss: 2.100109\n",
      "Train Epoch: 28 [12800/50000 (25.6%)]\tLoss: 2.104666\n",
      "Train Epoch: 28 [19200/50000 (38.4%)]\tLoss: 2.282326\n",
      "Train Epoch: 28 [25600/50000 (51.2%)]\tLoss: 2.655311\n",
      "Train Epoch: 28 [32000/50000 (63.9%)]\tLoss: 2.213496\n",
      "Train Epoch: 28 [38400/50000 (76.7%)]\tLoss: 2.302264\n",
      "Train Epoch: 28 [44800/50000 (89.5%)]\tLoss: 2.267485\n",
      "\n",
      "Test set: Average loss: 2.3720, Accuracy: 3851/10000 (38.5%)\n",
      "\n",
      "Train Epoch: 29 [0/50000 (0.0%)]\tLoss: 2.335333\n",
      "Train Epoch: 29 [6400/50000 (12.8%)]\tLoss: 2.504987\n",
      "Train Epoch: 29 [12800/50000 (25.6%)]\tLoss: 2.313980\n",
      "Train Epoch: 29 [19200/50000 (38.4%)]\tLoss: 2.169248\n",
      "Train Epoch: 29 [25600/50000 (51.2%)]\tLoss: 2.315537\n",
      "Train Epoch: 29 [32000/50000 (63.9%)]\tLoss: 2.402885\n",
      "Train Epoch: 29 [38400/50000 (76.7%)]\tLoss: 2.738656\n",
      "Train Epoch: 29 [44800/50000 (89.5%)]\tLoss: 2.384989\n",
      "\n",
      "Test set: Average loss: 2.3420, Accuracy: 3859/10000 (38.6%)\n",
      "\n",
      "Train Epoch: 30 [0/50000 (0.0%)]\tLoss: 2.660150\n",
      "Train Epoch: 30 [6400/50000 (12.8%)]\tLoss: 2.456942\n",
      "Train Epoch: 30 [12800/50000 (25.6%)]\tLoss: 2.057041\n",
      "Train Epoch: 30 [19200/50000 (38.4%)]\tLoss: 2.186847\n",
      "Train Epoch: 30 [25600/50000 (51.2%)]\tLoss: 2.518104\n",
      "Train Epoch: 30 [32000/50000 (63.9%)]\tLoss: 2.326478\n",
      "Train Epoch: 30 [38400/50000 (76.7%)]\tLoss: 2.167514\n",
      "Train Epoch: 30 [44800/50000 (89.5%)]\tLoss: 2.508549\n",
      "\n",
      "Test set: Average loss: 2.3144, Accuracy: 3902/10000 (39.0%)\n",
      "\n",
      "Train Epoch: 31 [0/50000 (0.0%)]\tLoss: 2.107127\n",
      "Train Epoch: 31 [6400/50000 (12.8%)]\tLoss: 2.346220\n",
      "Train Epoch: 31 [12800/50000 (25.6%)]\tLoss: 2.374313\n",
      "Train Epoch: 31 [19200/50000 (38.4%)]\tLoss: 2.288007\n",
      "Train Epoch: 31 [25600/50000 (51.2%)]\tLoss: 1.780675\n",
      "Train Epoch: 31 [32000/50000 (63.9%)]\tLoss: 2.613750\n",
      "Train Epoch: 31 [38400/50000 (76.7%)]\tLoss: 1.989826\n",
      "Train Epoch: 31 [44800/50000 (89.5%)]\tLoss: 2.523693\n",
      "\n",
      "Test set: Average loss: 2.3376, Accuracy: 3898/10000 (39.0%)\n",
      "\n",
      "Train Epoch: 32 [0/50000 (0.0%)]\tLoss: 2.010862\n",
      "Train Epoch: 32 [6400/50000 (12.8%)]\tLoss: 2.075515\n",
      "Train Epoch: 32 [12800/50000 (25.6%)]\tLoss: 2.545719\n",
      "Train Epoch: 32 [19200/50000 (38.4%)]\tLoss: 2.302973\n",
      "Train Epoch: 32 [25600/50000 (51.2%)]\tLoss: 2.257799\n",
      "Train Epoch: 32 [32000/50000 (63.9%)]\tLoss: 2.010404\n",
      "Train Epoch: 32 [38400/50000 (76.7%)]\tLoss: 2.016109\n",
      "Train Epoch: 32 [44800/50000 (89.5%)]\tLoss: 2.661571\n",
      "\n",
      "Test set: Average loss: 2.2766, Accuracy: 4005/10000 (40.0%)\n",
      "\n",
      "Train Epoch: 33 [0/50000 (0.0%)]\tLoss: 2.097452\n",
      "Train Epoch: 33 [6400/50000 (12.8%)]\tLoss: 2.439122\n",
      "Train Epoch: 33 [12800/50000 (25.6%)]\tLoss: 2.305373\n",
      "Train Epoch: 33 [19200/50000 (38.4%)]\tLoss: 1.736237\n",
      "Train Epoch: 33 [25600/50000 (51.2%)]\tLoss: 2.146689\n",
      "Train Epoch: 33 [32000/50000 (63.9%)]\tLoss: 2.214525\n",
      "Train Epoch: 33 [38400/50000 (76.7%)]\tLoss: 2.065642\n",
      "Train Epoch: 33 [44800/50000 (89.5%)]\tLoss: 2.546276\n",
      "\n",
      "Test set: Average loss: 2.2918, Accuracy: 4031/10000 (40.3%)\n",
      "\n",
      "Train Epoch: 34 [0/50000 (0.0%)]\tLoss: 2.035005\n",
      "Train Epoch: 34 [6400/50000 (12.8%)]\tLoss: 1.832240\n",
      "Train Epoch: 34 [12800/50000 (25.6%)]\tLoss: 2.377508\n",
      "Train Epoch: 34 [19200/50000 (38.4%)]\tLoss: 2.118986\n",
      "Train Epoch: 34 [25600/50000 (51.2%)]\tLoss: 2.451165\n",
      "Train Epoch: 34 [32000/50000 (63.9%)]\tLoss: 2.084453\n",
      "Train Epoch: 34 [38400/50000 (76.7%)]\tLoss: 2.236582\n",
      "Train Epoch: 34 [44800/50000 (89.5%)]\tLoss: 1.814778\n",
      "\n",
      "Test set: Average loss: 2.2623, Accuracy: 4010/10000 (40.1%)\n",
      "\n",
      "Train Epoch: 35 [0/50000 (0.0%)]\tLoss: 2.350976\n",
      "Train Epoch: 35 [6400/50000 (12.8%)]\tLoss: 2.346192\n",
      "Train Epoch: 35 [12800/50000 (25.6%)]\tLoss: 1.811272\n",
      "Train Epoch: 35 [19200/50000 (38.4%)]\tLoss: 2.079571\n",
      "Train Epoch: 35 [25600/50000 (51.2%)]\tLoss: 2.196026\n",
      "Train Epoch: 35 [32000/50000 (63.9%)]\tLoss: 2.248245\n",
      "Train Epoch: 35 [38400/50000 (76.7%)]\tLoss: 2.118338\n",
      "Train Epoch: 35 [44800/50000 (89.5%)]\tLoss: 2.122826\n",
      "\n",
      "Test set: Average loss: 2.2040, Accuracy: 4171/10000 (41.7%)\n",
      "\n",
      "Train Epoch: 36 [0/50000 (0.0%)]\tLoss: 2.270337\n",
      "Train Epoch: 36 [6400/50000 (12.8%)]\tLoss: 2.088643\n",
      "Train Epoch: 36 [12800/50000 (25.6%)]\tLoss: 2.149105\n",
      "Train Epoch: 36 [19200/50000 (38.4%)]\tLoss: 1.827198\n",
      "Train Epoch: 36 [25600/50000 (51.2%)]\tLoss: 2.048621\n",
      "Train Epoch: 36 [32000/50000 (63.9%)]\tLoss: 1.881992\n",
      "Train Epoch: 36 [38400/50000 (76.7%)]\tLoss: 1.903437\n",
      "Train Epoch: 36 [44800/50000 (89.5%)]\tLoss: 1.823444\n",
      "\n",
      "Test set: Average loss: 2.2068, Accuracy: 4190/10000 (41.9%)\n",
      "\n",
      "Train Epoch: 37 [0/50000 (0.0%)]\tLoss: 1.912168\n",
      "Train Epoch: 37 [6400/50000 (12.8%)]\tLoss: 1.974759\n",
      "Train Epoch: 37 [12800/50000 (25.6%)]\tLoss: 1.984429\n",
      "Train Epoch: 37 [19200/50000 (38.4%)]\tLoss: 2.220001\n",
      "Train Epoch: 37 [25600/50000 (51.2%)]\tLoss: 1.940290\n",
      "Train Epoch: 37 [32000/50000 (63.9%)]\tLoss: 1.918942\n",
      "Train Epoch: 37 [38400/50000 (76.7%)]\tLoss: 1.980500\n",
      "Train Epoch: 37 [44800/50000 (89.5%)]\tLoss: 2.285472\n",
      "\n",
      "Test set: Average loss: 2.1456, Accuracy: 4297/10000 (43.0%)\n",
      "\n",
      "Train Epoch: 38 [0/50000 (0.0%)]\tLoss: 2.031564\n",
      "Train Epoch: 38 [6400/50000 (12.8%)]\tLoss: 1.828301\n",
      "Train Epoch: 38 [12800/50000 (25.6%)]\tLoss: 2.042835\n",
      "Train Epoch: 38 [19200/50000 (38.4%)]\tLoss: 1.853262\n",
      "Train Epoch: 38 [25600/50000 (51.2%)]\tLoss: 1.635992\n",
      "Train Epoch: 38 [32000/50000 (63.9%)]\tLoss: 2.044514\n",
      "Train Epoch: 38 [38400/50000 (76.7%)]\tLoss: 1.735441\n",
      "Train Epoch: 38 [44800/50000 (89.5%)]\tLoss: 1.974198\n",
      "\n",
      "Test set: Average loss: 2.2014, Accuracy: 4220/10000 (42.2%)\n",
      "\n",
      "Train Epoch: 39 [0/50000 (0.0%)]\tLoss: 1.991354\n",
      "Train Epoch: 39 [6400/50000 (12.8%)]\tLoss: 2.121664\n",
      "Train Epoch: 39 [12800/50000 (25.6%)]\tLoss: 1.963917\n",
      "Train Epoch: 39 [19200/50000 (38.4%)]\tLoss: 1.979986\n",
      "Train Epoch: 39 [25600/50000 (51.2%)]\tLoss: 2.244589\n",
      "Train Epoch: 39 [32000/50000 (63.9%)]\tLoss: 2.232441\n",
      "Train Epoch: 39 [38400/50000 (76.7%)]\tLoss: 2.283286\n",
      "Train Epoch: 39 [44800/50000 (89.5%)]\tLoss: 2.072405\n",
      "\n",
      "Test set: Average loss: 2.1864, Accuracy: 4274/10000 (42.7%)\n",
      "\n",
      "Train Epoch: 40 [0/50000 (0.0%)]\tLoss: 2.075971\n",
      "Train Epoch: 40 [6400/50000 (12.8%)]\tLoss: 2.154820\n",
      "Train Epoch: 40 [12800/50000 (25.6%)]\tLoss: 1.819692\n",
      "Train Epoch: 40 [19200/50000 (38.4%)]\tLoss: 1.946419\n",
      "Train Epoch: 40 [25600/50000 (51.2%)]\tLoss: 1.843499\n",
      "Train Epoch: 40 [32000/50000 (63.9%)]\tLoss: 1.561180\n",
      "Train Epoch: 40 [38400/50000 (76.7%)]\tLoss: 1.724344\n",
      "Train Epoch: 40 [44800/50000 (89.5%)]\tLoss: 1.963165\n",
      "\n",
      "Test set: Average loss: 2.1251, Accuracy: 4363/10000 (43.6%)\n",
      "\n",
      "Train Epoch: 41 [0/50000 (0.0%)]\tLoss: 1.950875\n",
      "Train Epoch: 41 [6400/50000 (12.8%)]\tLoss: 2.212464\n",
      "Train Epoch: 41 [12800/50000 (25.6%)]\tLoss: 1.864150\n",
      "Train Epoch: 41 [19200/50000 (38.4%)]\tLoss: 1.705910\n",
      "Train Epoch: 41 [25600/50000 (51.2%)]\tLoss: 2.195621\n",
      "Train Epoch: 41 [32000/50000 (63.9%)]\tLoss: 1.768282\n",
      "Train Epoch: 41 [38400/50000 (76.7%)]\tLoss: 2.134766\n",
      "Train Epoch: 41 [44800/50000 (89.5%)]\tLoss: 1.931438\n",
      "\n",
      "Test set: Average loss: 2.1441, Accuracy: 4394/10000 (43.9%)\n",
      "\n",
      "Train Epoch: 42 [0/50000 (0.0%)]\tLoss: 2.017416\n",
      "Train Epoch: 42 [6400/50000 (12.8%)]\tLoss: 1.850673\n",
      "Train Epoch: 42 [12800/50000 (25.6%)]\tLoss: 2.137621\n",
      "Train Epoch: 42 [19200/50000 (38.4%)]\tLoss: 1.537033\n",
      "Train Epoch: 42 [25600/50000 (51.2%)]\tLoss: 1.926637\n",
      "Train Epoch: 42 [32000/50000 (63.9%)]\tLoss: 2.032623\n",
      "Train Epoch: 42 [38400/50000 (76.7%)]\tLoss: 1.697289\n",
      "Train Epoch: 42 [44800/50000 (89.5%)]\tLoss: 2.242747\n",
      "\n",
      "Test set: Average loss: 2.1135, Accuracy: 4382/10000 (43.8%)\n",
      "\n",
      "Train Epoch: 43 [0/50000 (0.0%)]\tLoss: 2.099694\n",
      "Train Epoch: 43 [6400/50000 (12.8%)]\tLoss: 1.943423\n",
      "Train Epoch: 43 [12800/50000 (25.6%)]\tLoss: 1.949966\n",
      "Train Epoch: 43 [19200/50000 (38.4%)]\tLoss: 1.864444\n",
      "Train Epoch: 43 [25600/50000 (51.2%)]\tLoss: 2.160171\n",
      "Train Epoch: 43 [32000/50000 (63.9%)]\tLoss: 1.601295\n",
      "Train Epoch: 43 [38400/50000 (76.7%)]\tLoss: 1.883151\n",
      "Train Epoch: 43 [44800/50000 (89.5%)]\tLoss: 1.892257\n",
      "\n",
      "Test set: Average loss: 2.1274, Accuracy: 4399/10000 (44.0%)\n",
      "\n",
      "Train Epoch: 44 [0/50000 (0.0%)]\tLoss: 1.577047\n",
      "Train Epoch: 44 [6400/50000 (12.8%)]\tLoss: 1.931623\n",
      "Train Epoch: 44 [12800/50000 (25.6%)]\tLoss: 2.099977\n",
      "Train Epoch: 44 [19200/50000 (38.4%)]\tLoss: 1.941725\n",
      "Train Epoch: 44 [25600/50000 (51.2%)]\tLoss: 2.057765\n",
      "Train Epoch: 44 [32000/50000 (63.9%)]\tLoss: 1.855928\n",
      "Train Epoch: 44 [38400/50000 (76.7%)]\tLoss: 1.912993\n",
      "Train Epoch: 44 [44800/50000 (89.5%)]\tLoss: 2.181729\n",
      "\n",
      "Test set: Average loss: 2.0524, Accuracy: 4625/10000 (46.2%)\n",
      "\n",
      "Train Epoch: 45 [0/50000 (0.0%)]\tLoss: 2.361468\n",
      "Train Epoch: 45 [6400/50000 (12.8%)]\tLoss: 1.868034\n",
      "Train Epoch: 45 [12800/50000 (25.6%)]\tLoss: 1.883850\n",
      "Train Epoch: 45 [19200/50000 (38.4%)]\tLoss: 2.077755\n",
      "Train Epoch: 45 [25600/50000 (51.2%)]\tLoss: 1.998984\n",
      "Train Epoch: 45 [32000/50000 (63.9%)]\tLoss: 1.969854\n",
      "Train Epoch: 45 [38400/50000 (76.7%)]\tLoss: 1.833957\n",
      "Train Epoch: 45 [44800/50000 (89.5%)]\tLoss: 1.812675\n",
      "\n",
      "Test set: Average loss: 2.0727, Accuracy: 4502/10000 (45.0%)\n",
      "\n",
      "Train Epoch: 46 [0/50000 (0.0%)]\tLoss: 1.933584\n",
      "Train Epoch: 46 [6400/50000 (12.8%)]\tLoss: 1.585969\n",
      "Train Epoch: 46 [12800/50000 (25.6%)]\tLoss: 1.409210\n",
      "Train Epoch: 46 [19200/50000 (38.4%)]\tLoss: 2.234208\n",
      "Train Epoch: 46 [25600/50000 (51.2%)]\tLoss: 2.227928\n",
      "Train Epoch: 46 [32000/50000 (63.9%)]\tLoss: 2.030704\n",
      "Train Epoch: 46 [38400/50000 (76.7%)]\tLoss: 1.587920\n",
      "Train Epoch: 46 [44800/50000 (89.5%)]\tLoss: 1.748789\n",
      "\n",
      "Test set: Average loss: 2.0349, Accuracy: 4597/10000 (46.0%)\n",
      "\n",
      "Train Epoch: 47 [0/50000 (0.0%)]\tLoss: 1.805533\n",
      "Train Epoch: 47 [6400/50000 (12.8%)]\tLoss: 1.948081\n",
      "Train Epoch: 47 [12800/50000 (25.6%)]\tLoss: 2.012455\n",
      "Train Epoch: 47 [19200/50000 (38.4%)]\tLoss: 1.975336\n",
      "Train Epoch: 47 [25600/50000 (51.2%)]\tLoss: 1.653564\n",
      "Train Epoch: 47 [32000/50000 (63.9%)]\tLoss: 2.142081\n",
      "Train Epoch: 47 [38400/50000 (76.7%)]\tLoss: 1.867957\n",
      "Train Epoch: 47 [44800/50000 (89.5%)]\tLoss: 1.707417\n",
      "\n",
      "Test set: Average loss: 2.0416, Accuracy: 4650/10000 (46.5%)\n",
      "\n",
      "Train Epoch: 48 [0/50000 (0.0%)]\tLoss: 1.632211\n",
      "Train Epoch: 48 [6400/50000 (12.8%)]\tLoss: 2.024253\n",
      "Train Epoch: 48 [12800/50000 (25.6%)]\tLoss: 1.987281\n",
      "Train Epoch: 48 [19200/50000 (38.4%)]\tLoss: 1.595933\n",
      "Train Epoch: 48 [25600/50000 (51.2%)]\tLoss: 1.599254\n",
      "Train Epoch: 48 [32000/50000 (63.9%)]\tLoss: 1.866983\n",
      "Train Epoch: 48 [38400/50000 (76.7%)]\tLoss: 1.831165\n",
      "Train Epoch: 48 [44800/50000 (89.5%)]\tLoss: 1.725896\n",
      "\n",
      "Test set: Average loss: 1.9917, Accuracy: 4702/10000 (47.0%)\n",
      "\n",
      "Train Epoch: 49 [0/50000 (0.0%)]\tLoss: 1.872323\n",
      "Train Epoch: 49 [6400/50000 (12.8%)]\tLoss: 1.775114\n",
      "Train Epoch: 49 [12800/50000 (25.6%)]\tLoss: 2.212043\n",
      "Train Epoch: 49 [19200/50000 (38.4%)]\tLoss: 1.919202\n",
      "Train Epoch: 49 [25600/50000 (51.2%)]\tLoss: 1.278111\n",
      "Train Epoch: 49 [32000/50000 (63.9%)]\tLoss: 1.554240\n",
      "Train Epoch: 49 [38400/50000 (76.7%)]\tLoss: 1.930891\n",
      "Train Epoch: 49 [44800/50000 (89.5%)]\tLoss: 1.917270\n",
      "\n",
      "Test set: Average loss: 1.9913, Accuracy: 4719/10000 (47.2%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_prec1 = 0.\n",
    "epochs = 50\n",
    "model.train()\n",
    "model.to(device)\n",
    "for epoch in range(0, epochs):\n",
    "    train(epoch)\n",
    "    prec1 = test()\n",
    "    is_best = prec1 > best_prec1\n",
    "    best_prec1 = max(prec1, best_prec1)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec1': best_prec1,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'cfg': model.cfg\n",
    "    }, is_best, filepath=save)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T02:25:48.418704Z",
     "iopub.status.busy": "2024-12-12T02:25:48.417881Z",
     "iopub.status.idle": "2024-12-12T02:27:19.268278Z",
     "shell.execute_reply": "2024-12-12T02:27:19.267373Z",
     "shell.execute_reply.started": "2024-12-12T02:25:48.418664Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after Finetuning pruned Model\n",
      "\n",
      "Test set: Average loss: 1.9913, Accuracy: 4719/10000 (47.2%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy after Finetuning pruned Model')\n",
    "acc = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T02:27:19.270714Z",
     "iopub.status.busy": "2024-12-12T02:27:19.270324Z",
     "iopub.status.idle": "2024-12-12T02:27:24.770159Z",
     "shell.execute_reply": "2024-12-12T02:27:24.769372Z",
     "shell.execute_reply.started": "2024-12-12T02:27:19.270670Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference         0.00%       0.000us         0.00%       0.000us       0.000us     169.943ms        57.74%     169.943ms      16.994ms            10  \n",
      "                                        model_inference         9.87%      47.752ms        36.04%     174.336ms      17.434ms       0.000us         0.00%     123.699ms      12.370ms            10  \n",
      "                                           aten::conv2d         0.38%       1.852ms         8.49%      41.057ms      83.790us       0.000us         0.00%      91.788ms     187.321us           490  \n",
      "                                      aten::convolution         0.98%       4.722ms         8.11%      39.205ms      80.010us       0.000us         0.00%      91.788ms     187.321us           490  \n",
      "                                     aten::_convolution         0.70%       3.372ms         7.13%      34.483ms      70.373us       0.000us         0.00%      91.788ms     187.321us           490  \n",
      "                                aten::cudnn_convolution         4.05%      19.576ms         6.43%      31.111ms      63.492us      91.788ms        31.19%      91.788ms     187.321us           490  \n",
      "       cudnn_infer_volta_scudnn_128x32_relu_small_nn_v1         0.00%       0.000us         0.00%       0.000us       0.000us      74.297ms        25.24%      74.297ms     225.143us           330  \n",
      "                                       aten::batch_norm         0.24%       1.182ms         8.11%      39.208ms      80.016us       0.000us         0.00%      13.277ms      27.096us           490  \n",
      "                           aten::_batch_norm_impl_index         0.43%       2.067ms         7.86%      38.026ms      77.604us       0.000us         0.00%      13.277ms      27.096us           490  \n",
      "                                 aten::cudnn_batch_norm         3.28%      15.870ms         7.43%      35.959ms      73.385us      13.277ms         4.51%      13.277ms      27.096us           490  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 483.673ms\n",
      "Self CUDA time total: 294.317ms\n",
      "\n",
      "\n",
      "Latency per batch: 0.0168 seconds\n",
      "Throughput: 3808.4054 samples/second\n",
      "Peak GPU memory usage: 495.03 MB\n",
      "Model parameter count: 761652\n",
      "FLOPs estimation: 1.52e+06 FLOPs\n"
     ]
    }
   ],
   "source": [
    "profile_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 189030,
     "modelInstanceId": 166710,
     "sourceId": 195513,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
